{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Projects/ASTRAL/lidar`\n"
     ]
    }
   ],
   "source": [
    "using Revise\n",
    "using Pkg; Pkg.activate(\".\")\n",
    "\n",
    "using Dates\n",
    "using Statistics\n",
    "using Interpolations\n",
    "using DSP\n",
    "using FFTW\n",
    "using NCDatasets\n",
    "using JLD2\n",
    "using Printf\n",
    "\n",
    "# include(\"./readers.jl\")\n",
    "include(\"./read_lidar.jl\")\n",
    "using .read_lidar\n",
    "# using MAT\n",
    "\n",
    "using PyPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "remove_coherent_component"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# utility functions\n",
    "pd = permutedims\n",
    "m2n(x) = ismissing(x) ? NaN : x\n",
    "\n",
    "\"bin average y(x) in bins b of coordinate x\"\n",
    "function binavg(y, x, b)\n",
    "    a = zeros(length(b))\n",
    "    c = zeros(length(b))\n",
    "    for (i,x) in enumerate(x)\n",
    "        bi = findlast(j -> j < x, b)\n",
    "        a[bi] += y[i]\n",
    "        c[bi] += 1\n",
    "    end\n",
    "    return a./c\n",
    "end\n",
    "\n",
    "# functions for masking and averaging data\n",
    "\n",
    "\"NaN -> missing\"\n",
    "n2m(x) = isfinite.(x) ? x : missing\n",
    "\n",
    "\"result is x; set to missing iff i<thr\"\n",
    "masklowi(x, i, thr=1.03) = i<thr ? missing : x\n",
    "\n",
    "\"mean along dimension dims, skipping missing\"\n",
    "missmean(X; dims=1) = mapslices(x -> mean(skipmissing(x)), X, dims=dims)\n",
    "\n",
    "\"anomaly\"\n",
    "anom(x; dims=1) = x.-mean(x; dims=dims)\n",
    "\n",
    "# highpass filter\n",
    "\"\"\"\n",
    "hp(x, fcutoff=1/80)    highpass filter x,\n",
    "by default filtfilt 4th-order Butterworth, fs=1\n",
    "\"\"\"\n",
    "function hp(x, fcutoff=1/80;\n",
    "    order=4,\n",
    "    designmethod=Butterworth(order), \n",
    "    fs=1,\n",
    "    responsetype = Highpass(fcutoff; fs=fs) )\n",
    "    \n",
    "    filtfilt(digitalfilter(responsetype, designmethod), x)\n",
    "end\n",
    "\n",
    "\n",
    "# make simple linear temporal interpolation\n",
    "# maybe fast\n",
    "# most time is spent searching for indices\n",
    "# indices are monotonic\n",
    "\n",
    "\"find indices i such that each xl[i] is the first >= xs.\"\n",
    "function findindices(xs, xl)\n",
    "    # xs needles define quarries in haystack xl\n",
    "    xs = filter(x -> x<=last(xl), xs) # prefilter to avoid running off the end of xl\n",
    "    ind = zeros(Int64, size(xs))\n",
    "    i = 1\n",
    "    for (j,x) in enumerate(xs)\n",
    "        while xl[i] < x\n",
    "            i += 1\n",
    "        end\n",
    "        ind[j] = i\n",
    "    end\n",
    "    return ind\n",
    "end\n",
    "\n",
    "\n",
    "# # remove old method for centered average - SPd 2024-11-11\n",
    "# \"average xl centered within +-half points of the index of xl\"\n",
    "# function indavg(xl, ind; half=10)\n",
    "#     xm = zeros(Float64, size(ind))\n",
    "#     for (i,idx) in enumerate(ind)\n",
    "#         ii = max(1,idx-half) : min(length(xl),idx+half)\n",
    "#         xm[i] = sum(Float64.(xl[ii])) / (2*half+1)\n",
    "#     end\n",
    "#     return xm\n",
    "# end\n",
    "# # replace with average to right of ind\n",
    "\"average xl within windows to right of points of the index ind of xl\"\n",
    "function indavg(xl, ind; full=20)\n",
    "    xm = zeros(Float64, size(ind))\n",
    "    for (i,idx) in enumerate(ind)\n",
    "        ii = max(1,idx) : min(length(xl),idx+full)\n",
    "        # xm[i] = sum(Float64.(xl[ii])) / (full+1)\n",
    "        xm[i] = mean(Float64.(xl[ii]))\n",
    "    end\n",
    "    return xm\n",
    "end\n",
    "\n",
    "# test data (precompiles)\n",
    "let xl = 1:60_000_000, xs = 20:20:60_000_000\n",
    "    ind = findindices(xs, xl)\n",
    "    indavg(xl, ind)\n",
    "end\n",
    "\n",
    "# Adjust true vertical velocity for relative wind * sin(tilt)\n",
    "# and the platform velocity\n",
    "trigs(pitch, roll) = ( cos(pitch), sin(pitch), cos(roll), sin(roll) )\n",
    "# cospitch, sinpitch, cosroll, sinroll = trigs(pitch, roll)\n",
    "\n",
    "function wtrue(w, Ur, Vr, pitch, roll)\n",
    "    cospitch, sinpitch, cosroll, sinroll = trigs(pitch, roll)\n",
    "    wtrue = ( w + Ur*sinpitch - Vr*cospitch*sinroll ) / (cospitch*cosroll)\n",
    "end\n",
    "\n",
    "# displacements with no adjustment for tilting into the horizontal wind \n",
    "# U, V vary slowly; pitch,roll,w vary fast\n",
    "# there are nt*(nt-1)/2 ~ O(nt^2) outputs, so correct stuff first\n",
    "\n",
    "# don't use:\n",
    "\"return the coherent component of signal1 and signal2\"\n",
    "function coherent_component(signal1::Vector{Float64}, signal2::Vector{Float64})\n",
    "    # Fourier Transform of the signals\n",
    "    S1 = fft(signal1)\n",
    "    S2 = fft(signal2)\n",
    "    \n",
    "    # Compute cross-spectral density\n",
    "    P12 = S1 .* conj(S2)\n",
    "    # P21 = conj(P12)\n",
    "    \n",
    "    # Compute auto-spectral density\n",
    "    P11 = S1 .* conj(S1)\n",
    "    P22 = S2 .* conj(S2)\n",
    "    \n",
    "    # Compute coherence\n",
    "    coherence = abs.(P12).^2 ./ (P11 .* P22)\n",
    "    \n",
    "    # Compute the coherent part\n",
    "    coherent_part_S1 = coherence .* S2\n",
    "    coherent_part_S2 = coherence .* S1\n",
    "    \n",
    "    # Inverse Fourier Transform to get the time-domain signals\n",
    "    coherent_signal1 = real(ifft(coherent_part_S1))\n",
    "    coherent_signal2 = real(ifft(coherent_part_S2))\n",
    "    \n",
    "    return coherent_signal1, coherent_signal2\n",
    "end\n",
    "\n",
    "\"remove the coherent component of signal1 and signal2\"\n",
    "function remove_coherent_component(signal1::Vector{Float64}, signal2::Vector{Float64})\n",
    "    # Fourier Transform of the signals\n",
    "    S1 = fft(signal1)\n",
    "    S2 = fft(signal2)\n",
    "    \n",
    "    # Compute cross-spectral density\n",
    "    P12 = S1 .* conj(S2)\n",
    "    # P21 = conj(P12)\n",
    "    \n",
    "    # Compute auto-spectral density\n",
    "    P11 = S1 .* conj(S1)\n",
    "    P22 = S2 .* conj(S2)\n",
    "    \n",
    "    # Compute coherence\n",
    "    coherence = abs.(P12).^2 ./ (P11 .* P22)\n",
    "    \n",
    "    # Compute the coherent part\n",
    "    coherent_part_S1 = coherence .* S2\n",
    "    coherent_part_S2 = coherence .* S1\n",
    "    \n",
    "    # Remove the coherent part\n",
    "    clean_S1 = S1 .- coherent_part_S1\n",
    "    clean_S2 = S2 .- coherent_part_S2\n",
    "    \n",
    "    # Inverse Fourier Transform to get the time-domain signals\n",
    "    clean_signal1 = real(ifft(clean_S1))\n",
    "    clean_signal2 = real(ifft(clean_S2))\n",
    "    \n",
    "    return clean_signal1, clean_signal2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CartesianIndex{2}[CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1)  …  CartesianIndex(996, 1), CartesianIndex(996, 1), CartesianIndex(996, 1), CartesianIndex(996, 1), CartesianIndex(997, 1), CartesianIndex(997, 1), CartesianIndex(997, 1), CartesianIndex(998, 1), CartesianIndex(998, 1), CartesianIndex(999, 1)], CartesianIndex{2}[CartesianIndex(2, 1), CartesianIndex(3, 1), CartesianIndex(4, 1), CartesianIndex(5, 1), CartesianIndex(6, 1), CartesianIndex(7, 1), CartesianIndex(8, 1), CartesianIndex(9, 1), CartesianIndex(10, 1), CartesianIndex(11, 1)  …  CartesianIndex(997, 1), CartesianIndex(998, 1), CartesianIndex(999, 1), CartesianIndex(1000, 1), CartesianIndex(998, 1), CartesianIndex(999, 1), CartesianIndex(1000, 1), CartesianIndex(999, 1), CartesianIndex(1000, 1), CartesianIndex(1000, 1)], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  499491, 499492, 499493, 499494, 499495, 499496, 499497, 499498, 499499, 499500], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  499491, 499492, 499493, 499494, 499495, 499496, 499497, 499498, 499499, 499500], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  996, 996, 996, 996, 997, 997, 997, 998, 998, 999], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11  …  997, 998, 999, 1000, 998, 999, 1000, 999, 1000, 1000], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# functions for structure functions\n",
    "\n",
    "# generate unique pairs of indices\n",
    "\"index unique pairs in a vector of length n\"\n",
    "function uniquepairs(n) \n",
    "    [ [l1, l2] for l1 in 1:n for l2 in (l1+1):n ]\n",
    "end\n",
    "\"index pairs of points in adjacent levels\"\n",
    "allcross(n) = [ [l1, l2] for l1 in 1:n for l2 in 1:n ]\n",
    "\n",
    "# beam geometry\n",
    "\"lidar beam range\"\n",
    "rng(iz, rangegate=24.0) = rangegate * (iz-1 + 0.5)\n",
    "\n",
    "\"\"\"\n",
    "compile indices of lidar volumes to be compared with\n",
    "structure functions\n",
    "\"\"\"\n",
    "function lidarindices(nt, nz, z1=1; nlevelstats=1)\n",
    "    if nlevelstats == 3\n",
    "        # The complete set that doesn't repeat pairs is \n",
    "        # 1 the complete set of nt*(n-1)/2 pairs for the top level (3)\n",
    "        # 2 the 2*nt*nt sets of pairs between every point in top (3) level and the next 2 levels\n",
    "        # Iteratively slide this box upward by 1 level for each level.\n",
    "    \n",
    "        # index pairs in middle level 2-2\n",
    "        up = uniquepairs(nt)\n",
    "        it1 = map(i->i[1], up) # time indices for pairs of point1, point2\n",
    "        it2 = map(i->i[2], up)\n",
    "        ci1_r22 = CartesianIndex.(tuple.(it1,z1)) # 1st point in pair lev\n",
    "        ci2_r22 = CartesianIndex.(tuple.(it2,z1)) # 2nd \n",
    "    \n",
    "        # index pairs of points from level 2-1, and 2-3\n",
    "        ac = allcross(nt)\n",
    "        it1 = map(i->i[1], ac)\n",
    "        it2 = map(i->i[2], ac)\n",
    "        ci1_r21 = ci1_r23 = CartesianIndex.(tuple.(it1,2))\n",
    "        ci2_r21 = CartesianIndex.(tuple.(it2,z1-1))\n",
    "        ci2_r23 = CartesianIndex.(tuple.(it2,z1+1))\n",
    "    \n",
    "        # omnibus set of cartesian index pairs for a level, including points in lev above and below\n",
    "        ci1 = [ci1_r23; ci1_r22; ci1_r21] # first of pairs\n",
    "        ci2 = [ci2_r23; ci2_r22; ci2_r21]\n",
    "        li1 = LinearIndices(ci1)\n",
    "        li2 = LinearIndices(ci2)\n",
    "        \n",
    "    elseif nlevelstats == 1\n",
    "        # just use structure function velocity pairs from one level of lidar range\n",
    "        up = uniquepairs(nt)\n",
    "        it1 = map(i->i[1], up) # time indices for pairs of point1, point2\n",
    "        it2 = map(i->i[2], up)\n",
    "        ci1_r11 = CartesianIndex.(tuple.(it1,z1)) # 1st point in pair lev\n",
    "        ci2_r11 = CartesianIndex.(tuple.(it2,z1)) # 2nd point in same lev\n",
    "    \n",
    "        # set of cartesian index pairs for a level, including points in lev above and below\n",
    "        ci1 = ci1_r11 # first of pairs\n",
    "        ci2 = ci2_r11\n",
    "        li1 = LinearIndices(ci1)\n",
    "        li2 = LinearIndices(ci2)\n",
    "    end\n",
    "    \n",
    "    it1 = map(idx->idx[1], ci1) #  t index of first point(s)\n",
    "    iz1 = map(idx->idx[2], ci1) #  z index of first\n",
    "    it2 = map(idx->idx[1], ci2) #  t       of second points(s)\n",
    "    iz2 = map(idx->idx[2], ci2) #  z          second\n",
    "    \n",
    "    return ci1,ci2, li1,li2, it1,iz1,it2,iz2\n",
    "end\n",
    "\n",
    "# try example\n",
    "ci1,ci2, li1,li2, it1,iz1,it2,iz2 = lidarindices(1000, 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rhopair"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displacments and structure functions \n",
    "\n",
    "rangegate = 24.0 # for ASTRAL 2024 Halo Photonics StreamLineXR\n",
    "\n",
    "\"\"\"\n",
    "zm, dr2, dz2, D2 = displacements( ci1,ci2, Udt,Vdt, pitch,roll, w; rangegate=rangegate)\n",
    "Displacements of sample pairs for one (vertical) subvolume.\n",
    "\"\"\"\n",
    "function displacements( ci1,ci2, Udt,Vdt, pitch,roll, w; rangegate=rangegate , timestep=timestep)\n",
    "    # get the individual indices\n",
    "    it1 = map(idx->idx[1], ci1) #  t index of first point(s)\n",
    "    iz1 = map(idx->idx[2], ci1) #  z index of first\n",
    "    it2 = map(idx->idx[1], ci2) #  t       of second points(s)\n",
    "    iz2 = map(idx->idx[2], ci2) #  z          second\n",
    "\n",
    "    rng(iz) = rangegate * (iz-1 + 0.5) # center of gates\n",
    "\n",
    "    # horiz translation of the sample volumes by mean wind\n",
    "    Udtbar = @. (Udt[iz2] + Udt[iz1]) / 2\n",
    "    Vdtbar = @. (Vdt[iz2] + Vdt[iz1]) / 2\n",
    "    X = @. Udtbar * (it2 - it1)\n",
    "    Y = @. Vdtbar * (it2 - it1)\n",
    "    # vertical middle of pair\n",
    "    zm = @. (rng(iz2) * cos(pitch[it2])*cos(roll[it2]) + rng(iz1) * cos(pitch[it1])*cos(roll[it1])) / 2\n",
    "    # displacement between pair of points\n",
    "    dz = @.     rng(iz2) * cos(pitch[it2])*cos(roll[it2]) - rng(iz1) * cos(pitch[it1])*cos(roll[it1])\n",
    "    dx = @. X + rng(iz2) *-sin(pitch[it2])                - rng(iz1) *-sin(pitch[it1])\n",
    "    dy = @. Y + rng(iz2) * cos(pitch[it2])*sin(roll[it2]) - rng(iz1) * cos(pitch[it1])*sin(roll[it1])\n",
    "    # distance between\n",
    "    dz2 = dz .* dz\n",
    "    dr2 = @. dz2 + dx*dx + dy*dy\n",
    "    # CORRECT W for HEAVE and for TILTING into the horizontal wind\n",
    "    # vel structure function\n",
    "    D2 = @. (w[ci2] - w[ci1])^2\n",
    "    # return properties of pairs\n",
    "    return zm, dr2, dz2, D2\n",
    "end\n",
    "\n",
    "\"dr^2/3 (1-(dz/dr)^2/4) displacement function for computing dissipation from structure function pairs\"\n",
    "rhopair(dr2, dz2) = dr2^(1/3) * (1 - dz2/(4*dr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_chunks"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# functions to find and use start and end indices of stare chunks\n",
    "\n",
    "# !MOVE utilities for requesting, reading, and subsetting data from files\n",
    "# TO read_lidar.jl\n",
    "# include(\"read_lidar.jl\")\n",
    "\n",
    "# Request chunk subscripts and start and end times in any time window.\n",
    "\"return time as a decimal hour from the start time\"\n",
    "function hour_beams(time, ibeam1, start_time)\n",
    "    # time is decimal hour of the day\n",
    "    # ibeam1 is the index of time corresponding to start_time\n",
    "    # start_time is the DateTime when a new file starts\n",
    "    \n",
    "    # expand time axis to include day and decimal hour\n",
    "    ibeam_expand = [ findlast( ibeam1 .<= i ) for i in eachindex(time) ]\n",
    "    dtbeams = @. floor(start_time, Day)[ibeam_expand] \n",
    "    hourbeams = @. 24*Dates.value(Day(dtbeams-dtbeams[1])) + time\n",
    "end\n",
    "\"hour_beams(timeangles::Dict)\"\n",
    "hour_beams(ta::Dict) = hour_beams(ta[:time], ta[:ibeam1], ta[:start_time])\n",
    "\n",
    "# identify indices of chunks from gaps\n",
    "function all_gaps(t)\n",
    "    ien = findall( diff(t) .> 0.01 ) # gap of more than a few tens of seconds\n",
    "    ist = ien .+ 1 # start of the next chunk\n",
    "    return ien, ist\n",
    "end\n",
    "\n",
    "function all_start_end_indices(ien, ist)\n",
    "    # ii increasing-order vector of all start and end indices\n",
    "    ii = sort( permutedims([ien ist])[:] ) # ordered; this should sort them, but sort() for good measure\n",
    "    jj = LinearIndices(ii) # jj indexes ii\n",
    "    # ii alternates end start-end ... start ii[jjen] == ien; ii[jjst] == ist\n",
    "    jjen = jj[1:2:end]\n",
    "    jjst = jj[2:2:end] # even starts of next chunks follow previous odd ends\n",
    "    # NOTE: a view can change the parity of starts and ends\n",
    "    return ii,jj, jjen,jjst\n",
    "end\n",
    "# ii is the big array of start and end indices\n",
    "\n",
    "# functions to check if an index i is an end point; or a start point\n",
    "isend(i) = in(i, Ref(ien)) # Boolean true for interval chunk end points of ii\n",
    "isstart(i) = in(i, Ref(ist)) # Boolean true for interval chunk start points of ii\n",
    "\n",
    "# NOTE ii default is computed by all_start_end_indices, but not available in the lexical scope\n",
    "# so ii MUST be provided in the call\n",
    "\n",
    "# end index corresponding to a start indes\n",
    "# get the index j such that i = ii[j]\n",
    "thisj(i, ii) = findfirst(ii.==i)\n",
    "# get the end index ien corresponding to the start index ist of the same chunk\n",
    "chunken(ist, ii) = ii[thisj(ist, ii) + 1]\n",
    "# get the start index ist corresp to the end index ien of the same chunk\n",
    "chunkst(ien, ii) = ii[thisj(ien, ii) - 1]\n",
    "\n",
    "# index the next chunk\n",
    "nextchunki(i, ii) = ii[thisj(i, ii) + 2]\n",
    "# index the previous chunk\n",
    "prevchunki(i, ii) = ii[thisj(i, ii) - 2]\n",
    "\n",
    "# Find chunks within query start and end times.\n",
    "\"\"\"\n",
    "Request start and end chunks based on time indices.\n",
    "Inclusive: include chunks for which query falls in the middle.\n",
    "\"\"\"\n",
    "function query_start_end_chunks(qst, qen; ien=ien, ist=ist)\n",
    "    ien1 = ien[findfirst(x-> x>=qst , ien)] # end index of first queried chunk     # do in this order\n",
    "    ist1 = ist[findlast( x-> x <ien1, ist)] # start index of first queried chunk\n",
    "    lastist = ist[findlast( x-> x<=qen   , ist)] # start index of last queried chunk\n",
    "    lastien = ien[findfirst(x-> x>lastist, ien)] # end index of last queried chunk\n",
    "    ist1,ien1, lastist,lastien\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "request all start indices and all end indices between \n",
    "index of start of first chunk and end of last chunk lastien\n",
    "\"\"\"\n",
    "function all_chunks(ist1, lastien, ii)\n",
    "    # pickets of queried subset\n",
    "    iisub = ii[ findall(x-> ist1<=x<=lastien, ii) ] # start-end ... start-end ; no orphans\n",
    "\n",
    "    istsub = iisub[1:2:end] # iisub parity switched from ii\n",
    "    iensub = iisub[2:2:end] # even ends follow odd starts of same chunk\n",
    "    return istsub, iensub # start-end pairs\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "read_lidar_chunks"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# functions to determine which file a chunk in and its indices in that file\n",
    "\n",
    "# !MOVE utilities for requesting, reading, and subsetting data from files\n",
    "# TO read_lidar.jl\n",
    "# include(\"read_lidar.jl\")\n",
    "\n",
    "# time index hierarchy:\n",
    "#\n",
    "# beam\n",
    "#   chunk [start, end]\n",
    "#       file [start, end]\n",
    "\n",
    "# Collect data start indices times of all files in ibeam1\n",
    "# number of beams in each file nbeam\n",
    "# so last beam index in a file is ibeam1+nbeam-1\n",
    "# get_start_fileidx(tidx) = findlast(t-> ibeam1<=t, tidx)\n",
    "# get_end_fileidx(tidx) = findfirst(t-> ibeam1+nbeam-1>=t, tidx)\n",
    "get_stare_files(tidx, files) = files[get_fileidx(tidx)]\n",
    "\n",
    "# vectors of all file start and end indices\n",
    "function get_all_file_start_end_idxs(ta::Dict)\n",
    "    fi1 = ta[:ibeam1][:]\n",
    "    filast = @. fi1 + ta[:nbeams] - 1\n",
    "    return fi1, filast\n",
    "end\n",
    "function get_all_file_start_end_idxs(file_paths) \n",
    "    ta = read_streamlinexr_beam_timeangles(file_path)\n",
    "    get_all_file_start_end_idxs(ta)\n",
    "end\n",
    "\n",
    "\"index of the file for which the index x falls between fi1 and filast\"\n",
    "thisfile_idx(x, fi1, filast) = findfirst( fi1.<= x .<=filast)\n",
    "\n",
    "# this method superseded by one that uses fi1, filast in calling scope\n",
    "idx_beams_in_file(i, ibeam1, fi1, filast) = i - ibeam1[thisfile_idx(i, fi1, filast)] + 1\n",
    "\n",
    "function thisfile_start_end(x, fi1, filast)\n",
    "    j = thisfile_idx(x, fi1, filast)\n",
    "    return fi1[j], filast[j]\n",
    "end\n",
    "\n",
    "\"get name of a file corresponding to a time or index\"\n",
    "thisfile_name(x::Integer, fi1, filast, files=fullfiles) = files[thisfile_idx(x, fi1, filast)]\n",
    "function thisfile_name(dtx::DateTime, dt::Vector{DateTime}, files=fullfiles) \n",
    "    fi1, filast = get_all_file_start_end_idxs(files)\n",
    "    thisfile_name( findlast(dt .<= dtx), fi1, filast, files)\n",
    "end\n",
    "\n",
    "\"read chunks inclusive between [firstdtq lastdq] from vector of files\"\n",
    "function read_lidar_chunks(files, firstdtq, lastdtq)\n",
    "    # read all times\n",
    "    ta, hdr, nbeams = read_lidar.read_streamlinexr_beam_timeangles(files) # 5 s per day\n",
    "\n",
    "    # TODO append day hour to the decimal hour...\n",
    "    hdr[:start_time]\n",
    "    # finding indices\n",
    "    ien, ist = all_gaps(ta[:time]) # identify indices of chunks from gaps\n",
    "    ii, jj = all_start_end_indices(ien, ist)\n",
    "\n",
    "    # Request some beam times. Low-level query takes indices.\n",
    "    # find indices qst, qen for the request\n",
    "    dt = @. DateTime(2024,5,31) + Millisecond(round(Int64, ta[:time] * 3_600_000)) # ta[:time] is decimal hour\n",
    "    # problem: date not defined in ta\n",
    "    qst = findfirst(dt .>= DateTime(2024,5,31,5))\n",
    "    qen = findlast( dt .<  DateTime(2024,5,31,10))\n",
    "\n",
    "    # request the first and last chunks\n",
    "    (ist1,ien1, lastist,lastien) = query_start_end_chunks(qst, qen; ien=ien, ist=ist)\n",
    "    # request start and end of all the chunks between ist1 and lastien\n",
    "    istsub, iensub = all_chunks(ist1, lastien)\n",
    "\n",
    "    # sort the chunks by file\n",
    "\n",
    "    # Subset the data within the file to the reuqested chunks\n",
    "    # could be done by skipping to start_beam, and stopping at stop_beam.\n",
    "    ibeam1 = ta[:ibeam1]\n",
    "    # not needed:\n",
    "    ## idx_beams_in_file(i, ibeam1) = i - ibeam1[thisfile_idx(i, ist1, lastien)] + 1\n",
    "    # start_beam = idx_beams_in_file(ist1, ibeam1, ist1, lastien) # default = 1\n",
    "    # stop_beam = idx_beams_in_file(lastien, ibeam1, ist1, lastien)      # default = ta[:nbeams]\n",
    "    \n",
    "    # only ever truncate the data read by a file because of a\n",
    "    # intentional scientific user-level query\n",
    "\n",
    "    # @show ist1, lastien\n",
    "    # @show [fi1 filast]\n",
    "\n",
    "    # read the beam data between ist1 and lastien\n",
    "    beams, h, nbeams0 = read_lidar.read_streamlinexr_stare(files, fi1, filast, ist1, lastien)\n",
    "    return beams, h, ien, ist\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol, Vector} with 8 entries:\n",
       "  :time       => Union{Missing, Float32}[0.00510278, 0.00538889, 0.00567222, 0.…\n",
       "  :start_time => Union{Missing, DateTime}[DateTime(\"2024-05-31T00:00:19.390\"), …\n",
       "  :roll       => Union{Missing, Float32}[-0.81, 0.01, -0.91, -1.32, -1.62, -2.6…\n",
       "  :elevangle  => Union{Missing, Float32}[90.01, 90.0, 90.0, 90.0, 90.0, 90.0, 9…\n",
       "  :nbeams     => Union{Missing, Int32}[3206, 3209, 3210, 3209, 3210, 3209, 3210…\n",
       "  :azimuth    => Union{Missing, Float32}[359.99, 0.0, 360.0, 0.0, 360.0, 0.0, 0…\n",
       "  :ibeam1     => Union{Missing, Int32}[1, 3207, 6416, 9626, 12835, 16045, 19254…\n",
       "  :pitch      => Union{Missing, Float32}[0.11, 0.72, 0.92, 1.13, 0.31, -0.3, -0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example to get indices of start and end of chunks for one day\n",
    "# tests functions proposed to MOVE to module read_lidar.chunks\n",
    "\n",
    "# include(\"read_lidar.jl\") # reloads readers\n",
    "\n",
    "# set up\n",
    "datapath = joinpath.(pwd(),\"data\",\"20240531\")\n",
    "files = filter(startswith(\"Stare\"), readdir(datapath))\n",
    "fullfiles = joinpath.(datapath, files)\n",
    "\n",
    "# to also read the first hour of the next day\n",
    "dtstamp = datapath[end-7:end]\n",
    "nextdt = DateTime(datapath[end-7:end], dateformat\"yyyymmdd\") + Day(1)\n",
    "nextdatapath = joinpath.(pwd(), \"data\", Dates.format(nextdt, \"yyyymmdd\"))\n",
    "hour00 = readdir( nextdatapath ) |> filter(startswith(\"Stare\")) |> filter(endswith(\"_00.hpl\"))\n",
    "full25files = [fullfiles ; joinpath(nextdatapath, hour00[1])]\n",
    "\n",
    "\n",
    "# read all times\n",
    "ta, hdr, nbeams = read_lidar.read_streamlinexr_beam_timeangles(full25files) # 5 s per day\n",
    "# ta\n",
    "# times, ibeam1, nbeams, h = read_lidar.read_streamlinexr_beam_times(fullfiles) # 5 s per day DEPRECATED\n",
    "fi1, filast = get_all_file_start_end_idxs(ta)\n",
    "\n",
    "hourbeams = hour_beams(ta) # decimal hour of each beam\n",
    "hourbeams[1:5] * 3600 # show seconds since start of day\n",
    "\n",
    "# find wrapped indices\n",
    "i20 = findfirst(ta[:time] .> 20.0)\n",
    "wrap = (i20-1) .+ findall( ta[:time][i20:end] .< 5 )\n",
    "tatime = ta[:time]\n",
    "# increment wrapped times by 24 h\n",
    "tatime[wrap] .+= 24.0\n",
    "# plot(tatime); gcf()\n",
    "\n",
    "# find indices\n",
    "ien, ist = all_gaps(tatime) # identify indices of chunks from gaps\n",
    "ii, jj = all_start_end_indices(ien, ist) ## jj is a LinearIndices\n",
    "\n",
    "# Request some beam times. Low-level query takes indices.\n",
    "# find indices qst, qen for the request\n",
    "dt = @. DateTime(2024,5,31) + Millisecond(round(Int64, ta[:time] * 3_600_000)) # ta[:time] is decimal hour\n",
    "# time wraps around at 0 UTC\n",
    "# problem: date not defined in ta\n",
    "qst = findfirst(dt .>= DateTime(2024,5,31,5))\n",
    "qen = findlast( dt .<  DateTime(2024,5,31,10))\n",
    "# try ones that wrap at 00Z\n",
    "qst = findfirst(dt .>= DateTime(2024,5,31,20))\n",
    "qen = findlast( dt .<  DateTime(2024,6, 1, 0,10)) # 10 min should be enough\n",
    "\n",
    "# get first and last indices of requested chunks\n",
    "(ist1,ien1, lastist,lastien) = query_start_end_chunks(qst, qen; ien=ien, ist=ist)\n",
    "# request start and end of all the chunks between ist1 and lastien\n",
    "istsub, iensub = all_chunks(ist1, lastien, ii)\n",
    "\n",
    "# sort the chunks by file\n",
    "\n",
    "# Subset the data within the file to the reuqested chunks\n",
    "# could be done by skipping to start_beam, and stopping at stop_beam.\n",
    "ibeam1 = ta[:ibeam1]\n",
    "# idx_beams_in_file(i, ibeam1) = i - ibeam1[thisfile_idx(i, fi1, filast)] + 1\n",
    "# start_beam = idx_beams_in_file(ist1   , ibeam1, fi1, filast) # default = 1\n",
    "# stop_beam  = idx_beams_in_file(lastien, ibeam1, fi1, filast) # default = ta[:nbeams]\n",
    "# only ever truncate the data read by a file because of a\n",
    "# intentional scientific user-level query\n",
    "\n",
    "# @show ist1, lastien\n",
    "# @show [fi1 filast]\n",
    "\n",
    "# read the beam data between ist1 and lastien\n",
    "beams, h, nbeams0 = read_lidar.read_streamlinexr_stare(full25files, fi1, filast, ist1, lastien)\n",
    "beams[:dopplervel]\n",
    "\n",
    "# show timeangles Dict\n",
    "ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunks do not end exactly at hourly files, or even at the turnover \n",
    "of the day. A way to deal with this is \n",
    " - Read `timeangles` in daily batches. \n",
    " - The queried data starts at 00 of the day, and ends at the end of the day,\n",
    "   or 00:00:00 of day+1.\n",
    " - Daily batches additionally include the following 00 hour of the next day \n",
    "   - This will provide for reading the\n",
    "   end of the last chunk that runs into the next day.\n",
    " - Read only queried data; there will be minimal overlap for reading \n",
    "   some of the last chunk in the 00 h day+1 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test identifying stare starts from a day of data\n",
    "length(ta[:time]) # == 77024\n",
    "ta[:start_time] # hourly start times\n",
    "# plot(ta[:elevangle],marker=\".\", linestyle=\"none\") # all stares ~90 degree elev\n",
    "starestart = findall(diff(ta[:time]).>(20/3600)) .+ 1\n",
    "\n",
    "clf()\n",
    "plot(ta[:time], marker=\".\", markersize=0.2, linestyle=\"none\") # hour offsets from day\n",
    "plot(starestart, ta[:time][starestart], marker=\".\", markersize=2, color=\"r\", linestyle=\"none\") # hour offsets from day\n",
    "# xlim([0, 7200])\n",
    "# ylim([0, 2])\n",
    "xlim([64000, lastien+60])\n",
    "ylim([20, 25])\n",
    "gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991-element Vector{DateTime}:\n",
       " 2024-04-28T00:00:00\n",
       " 2024-04-28T01:00:00\n",
       " 2024-04-28T02:00:00\n",
       " 2024-04-28T03:00:00\n",
       " 2024-04-28T04:00:00\n",
       " 2024-04-28T05:00:00\n",
       " 2024-04-28T06:00:00\n",
       " 2024-04-28T07:00:00\n",
       " 2024-04-28T08:00:00\n",
       " 2024-04-28T09:00:00\n",
       " ⋮\n",
       " 2024-06-12T22:00:00\n",
       " 2024-06-12T23:00:00\n",
       " 2024-06-13T00:00:00\n",
       " 2024-06-13T01:00:00\n",
       " 2024-06-13T02:00:00\n",
       " 2024-06-13T03:00:00\n",
       " 2024-06-13T04:00:00\n",
       " 2024-06-13T05:00:00\n",
       " 2024-06-13T06:00:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get all the files, and all the unique hours of the files\n",
    "allstarefiles = vcat( [ joinpath.(\"data\",F, \n",
    "    filter( startswith(r\"Stare_\"), readdir(joinpath(\"data\",F)) ) ) \n",
    "  for F in filter( startswith(r\"20240\"), readdir(\"data\") ) ]... )\n",
    "\n",
    "REm = match.(r\"Stare_116_(\\d{8}_\\d{2}).hpl\", allstarefiles)\n",
    "dth = [ DateTime(r[1], dateformat\"yyyymmdd_HH\") for r in REm ]\n",
    "unique(floor.(dth, Hour)) # all 991 are already unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## line up mean vertical velocity with platform heave\n",
    "using VectorNav - POSMV analysis in [`vectornav.ipynb`](vectornav.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectornav timing # MOVED to timing_lidar\n",
    "\n",
    "#=\n",
    "# Define the GPS epoch\n",
    "const GPS_EPOCH = DateTime(1980, 1, 6) # DateTime\n",
    "# const GPS_OFFSET = Dates.datetime2epochms(GPS_EPOCH) # Integer time interval since DateTime epoch\n",
    "\n",
    "# Function to calculate the number of leap seconds between two dates\n",
    "function leap_seconds(date::DateTime)\n",
    "    leap_seconds_list = [\n",
    "        DateTime(1981, 7, 1), DateTime(1982, 7, 1), DateTime(1983, 7, 1),\n",
    "        DateTime(1985, 7, 1), DateTime(1988, 1, 1), DateTime(1990, 1, 1),\n",
    "        DateTime(1991, 1, 1), DateTime(1992, 7, 1), DateTime(1993, 7, 1),\n",
    "        DateTime(1994, 7, 1), DateTime(1996, 1, 1), DateTime(1997, 7, 1),\n",
    "        DateTime(1999, 1, 1), DateTime(2006, 1, 1), DateTime(2009, 1, 1),\n",
    "        DateTime(2012, 7, 1), DateTime(2015, 7, 1), DateTime(2017, 1, 1)\n",
    "    ]\n",
    "    return count(ls -> ls <= date, leap_seconds_list)\n",
    "end\n",
    "\n",
    "# \"convert gpstime (nanoseconds) to DateTime\"\n",
    "# function gps2dt(gpstime::Integer)\n",
    "#     t_d = floor(gpstime / (86_400 * 1_000_000_000)) # nanosecond -> integer day\n",
    "#     t_ms = round(Int64, (gpstime % (86_400 * 1_000_000_000) ) / 1_000_000 - 1_000*leap_seconds(t_d)) # nansecond -> millisecond of day\n",
    "#     GPS_EPOCH + Day(t_d) + Millisecond(t_ms)\n",
    "# end\n",
    "\n",
    "\"convert gpstime (nanoseconds) to DateTime.\"\n",
    "function gps2dt(gpstime::Integer)\n",
    "    # leap seconds not needed when using time elapsed\n",
    "    # Millisecond(Integer) -> millisecond timedelta\n",
    "    GPS_EPOCH + Millisecond(round(Int64, gpstime / 1_000_000))\n",
    "end\n",
    "\n",
    "\"convert gpstime (nanoseconds) to DateTime epoch milliseconds (integer).\"\n",
    "function gps2ms(gpstime::Integer)\n",
    "    round(Int64, gpstime / 1_000_000 ) + Dates.datetime2epochms(GPS_EPOCH)\n",
    "end\n",
    "\n",
    "dt2gpsns(dt) = Dates.value(Millisecond( dt-GPS_EPOCH )) * 1_000_000 # -> Integer\n",
    "GPS_MORETHAN = dt2gpsns(DateTime(2024,1,1))\n",
    "\n",
    "# tests\n",
    "Dates.datetime2epochms(DateTime(0)) # 0 OK\n",
    "Dates.epochms2datetime( Dates.datetime2epochms(GPS_EPOCH) ) # 1980-01-06T00:00:00 OK\n",
    "gps2dt( dt2gpsns(DateTime(2024,4,29,5,25,42,128)) ) # 2024-04-29T05:25:42.128 OK\n",
    "DateTime(0) + Millisecond(gps2ms( dt2gpsns(DateTime(2024,4,29,5,25,42,128)) )) # 2024-04-29T05:25:42.128 OK\n",
    "DateTime(0) + Millisecond( 63881587542128 ) # # 2024-04-29T05:25:42.128 OK\n",
    "gps2ms( dt2gpsns(DateTime(2024,4,29,5,25,42,128)) ) # 63881587542128 OK\n",
    "\n",
    "\"\"\"\n",
    "itp(dtq) = interpolatedt(dtk, xk, Gridded(Linear()))(Dates.datetime2epochms(dtq))\n",
    "interpolate using datetimes as coordinates\n",
    "\"\"\"\n",
    "interpolatedt(dt, x...) = interpolate(Dates.datetime2epochms(dt), x...)\n",
    "\n",
    "\"short wrapper for Dates.value(Millisecond(t))\"\n",
    "DVM(t::TimePeriod) = Dates.value(Millisecond(t))\n",
    "DVM(t::Integer) = Dates.value(Millisecond(t))\n",
    "DVM(t::Real) = DVM(round(Int64,t))\n",
    "DVM(t::AbstractVector{T}) where T<:TimePeriod = DVM.(t)\n",
    "function DVM(dt::DateTime)\n",
    "    error(\"do not use DVM(::DateTime)\")\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Millisecond( round(Integer, v) )\n",
    "Convert a number to a Millisecond, rounding if necessary. Quasi-inverse of DVM.\n",
    "\"\"\"\n",
    "MRi(v::Number) = Millisecond( round(Integer, v) ) # convert a number to a Millisecond, rounding if necessary\n",
    "\n",
    "\"\"\"\n",
    "Interpolate between imprecisely recorded jumps in time.\n",
    "Need Float64 precision for interpolation\n",
    "\"\"\"\n",
    "function intbetweentime(prectime::Vector{Float64})\n",
    "    # monotonic, OK\n",
    "    ii0 = map(x-> x<=0, diff(prectime)) \n",
    "    # ind0 = boolean 1 for imprecise times that should be interpolated\n",
    "    ind0 = [0; findall(ii0)] .+ 1   # interpolate to these, increment because of diff()\n",
    "    ind1 = findall(.!ii0) .+ 1 # interpolate from these\n",
    "\n",
    "    # interpolate (and extrapolate) the inbetween times\n",
    "    itp = extrapolate( \n",
    "            interpolate((ind1,), prectime[ind1], Gridded(Linear())), \n",
    "            Line() )\n",
    "    # fltind0 = filter(i-> i>ind1[1] && i<ind1[end], ind0)\n",
    "    # prectime[fltind0] = itp(fltind0)\n",
    "    prectime[ind0] .= itp(ind0)\n",
    "    # or just just reinterp all indices\n",
    "    # prectime = itp(axes(prectime))\n",
    "    return prectime\n",
    "end\n",
    "\n",
    "\n",
    "# methods for averaging DateTimes and TimePeriod\n",
    "\n",
    "\"meandt(dt, ...)\"\n",
    "function meandt(dt::AbstractVector{DateTime}, args...)\n",
    "    ms = @. Dates.value(Millisecond(dt - dt[1]))\n",
    "    return dt[1] .+ Millisecond(round(Int64, mean(ms, args...)))\n",
    "end\n",
    "# \"mean for datetimes\" updated 2024-08-01\n",
    "# meandt(dt::AbstractVector{DateTime}) = first(dt) + MRi( mean(DVM.(dt .- first(dt))) )\n",
    "\"meandt(t, ...)\"\n",
    "function meandt(dt::AbstractVector{T}, args...) where T<:TimePeriod\n",
    "    ms = @. Dates.value(Millisecond(dt))\n",
    "    return Millisecond(round(Int64, mean(ms, args...)))\n",
    "end\n",
    "\n",
    "\"interpolate and add offsets to DateTime of precise time from vectornav.\"\n",
    "function precise_dt(vn_raw_dt, gpstime)\n",
    "    RASP_BASE_TIME = floor(vn_raw_dt[1], Dates.Day(1)) # totally bogus unset offset clock time\n",
    "    # need Float64 precision for interpolation\n",
    "    # RASP_BASE_TIME is a datetime before the start of the data.\n",
    "    \n",
    "    # add offsets\n",
    "    prectime = intbetweentime( Float64.(DVM( vn_raw_dt .- RASP_BASE_TIME )) )\n",
    "    precdt_vn_clock = RASP_BASE_TIME .+ Dates.Millisecond.(round.(Int64, prectime))\n",
    "    offset_gps_vn = gps2dt(gpstime[1]) - precdt_vn_clock[1] # OK for leg 1\n",
    "    precdt = precdt_vn_clock + offset_gps_vn # move to absolute GPS clock\n",
    "    return precdt\n",
    "    # next linearly stretch to minimize bias from GPS\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Stretch VectorNav millisecond-interpolated\n",
    "time linearly to match best GPS times at \n",
    "the start and 0-1 minute after the reset.\n",
    "\"\"\"\n",
    "function stretch_vn_to_gps(precdt, gpstime)\n",
    "    i0 = 2 # step starting good GPS times\n",
    "    ir = iregress(gpstime)[1]+1 # step after GPS reset\n",
    "    # delta = meandt(gps2dt.(gpstime[ir:(ir+1200)])) - meandt(precdt[ir:(ir+1200)])\n",
    "    delta = meandt(gps2dt.(gpstime[ir:(ir+200)])) - meandt(precdt[ir:(ir+200)])\n",
    "    buildup = range(start=0, length=length(precdt), step=DVM(delta)/(ir+600))\n",
    "    vntime = @. precdt + Millisecond(round(Int64,buildup))\n",
    "    return vntime # DateTime\n",
    "end\n",
    "\n",
    "\"shift, interp, and stretch a time axis to match the GPS\"\n",
    "gpstime2gpsvndt(time, gpstime) = stretch_vn_to_gps( precise_dt(time, gpstime), gpstime )\n",
    "\"subtract leap seconds to get UTC time from GPS time\"\n",
    "gps2utc(gpsdt) = gpsdt - Second(leap_seconds(DateTime(gpsdt)))\n",
    "gpstime2utcvndt(time, gpstime) = gps2utc( gpstime2gpsvndt(time, gpstime) )\n",
    "=#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data sets\n",
    "\n",
    "Procedures will differ for different legs. \n",
    "\n",
    "### Leg 1 \n",
    "Has VectorNav data, presumably physically aligned with the coordinate system of the lidar. At least its heave is aligned with the vertical lidar beams.\n",
    "\n",
    "VectorNav clock seems drift a few seconds and then correct, or precesses, with a 13-hour cycle, compared to the POSMV.\n",
    "\n",
    "![POSMV](./Vn-POSMV_lag.png \"VectorNav-POSMV lag\")\n",
    "\n",
    "The POSMV clock runs slow compared to its own internal GPS clock. In leg2 the clock resets\n",
    "to the GPS quasi-regularly every ~2 days (but never in leg 1). We reconstruct the POSMV time axis\n",
    "to minimize the difference with the GPS. This should agree with the VectorNav, but note\n",
    "the 18 GPS leapseconds as VectorNav time is in GPS convention, rather than UTC convention.\n",
    "\n",
    "![POSMV](./POSMV_pashr-gps.png \"VectorNav-POSMV lag\")\n",
    "\n",
    "### Leg 2, part 1\n",
    "There is no VectorNav data until June 4, when data starts.\n",
    "There is data from the ship's POSMV. Synchronization of pitch and roll\n",
    "is demonstrated in `vectornav.ipynb`.\n",
    "\n",
    "1. Load all _times_ for beams and for all VectorNav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D2_rho_stare"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# functions for epsilon from stare w\n",
    "# called in loop\n",
    "\n",
    "# timing functions\n",
    "\n",
    "\"\"\"\n",
    "find_time_shift(mdv, heave) positive result means mdv clock is fast.\n",
    "sync by subtracting this lag (in 1020 millisecods) from stare_dt.\n",
    "\"\"\"\n",
    "function get_time_shift(mdv, heave)\n",
    "    # filter to make xcorr work better\n",
    "    xc = xcorr(hp(mdv[:]), hp(heave[:]))\n",
    "    # plot(-(length(mdv)-1):length(mdv)-1, xc )\n",
    "    return argmax(xc) - length(mdv)\n",
    "end\n",
    "\n",
    "\"hourly files -> chunk time indices\"\n",
    "function read_stare_time( St )\n",
    "    # Lidar clock is fast (ahead) by 126832 milliseconds compared to the GPS.\n",
    "    # Moving the timeseries backward (lagging the lidar) compensates its clock error.\n",
    "    # adjust the lidar clock backward to agee with the GPS clock.\n",
    "    lidar_clock_fast_by = Millisecond( 126832 ) # first adjustment\n",
    "    stare_dt = @. (\n",
    "        DateTime(Date(dt)) \n",
    "        + Millisecond(round(Int64, St[:time] * 3_600_000 )) \n",
    "        .- lidar_clock_fast_by ) # 3202\n",
    "\n",
    "    # split into individual stare chunks\n",
    "    pickets = findall( t -> t>Second(30), diff(stare_dt) )\n",
    "    # st = [1; pickets.+1] # ignore start and end of file with a split chunk\n",
    "    # en = [pickets; length(stare_dt)]\n",
    "    st_chunk = pickets[1:end-1] .+ 1\n",
    "    en_chunk = pickets[2:end]\n",
    "    # subdivide into shorter chunks???\n",
    "    return st_chunk, en_chunk\n",
    "end\n",
    "\n",
    "# \"subdivide interval [st en] into fac even intervals\"\n",
    "# subdivide(st,en, fac) = round(Integer, st .+ (st-en)/fac .* [0:fac])\n",
    "\"subdivide single interval [st en] into fac even intervals\"\n",
    "subdivide(st,en, fac) = @. round(Integer, st + ((en-st)*(0:fac)/fac))\n",
    "# # test\n",
    "# subdivide(1,240, 4)\n",
    "\n",
    "\"read and interpolate data to stare chunks\"\n",
    "function read_stare_chunk(St, Vn, UV, st, en )\n",
    "    # time: truncate the file's datestamp to Date, add the decimal hour\n",
    "    stare_dt_raw = @. DateTime(Date(dt)) + Millisecond(round(Int64, St[:time] * 3_600_000 )) # 3202\n",
    "    lidar_clock_fast_by = Millisecond( 126832 ) # adjust for lidar clock fast (ahead) by 126832 milliseconds compared to the GPS.\n",
    "    stare_dt = stare_dt_raw .- lidar_clock_fast_by\n",
    "\n",
    "    # dopplervel (time, z) masked by intensity\n",
    "    dopplervel = masklowi.(St[:dopplervel][st:en,1:ntop], St[:intensity][st:en,1:ntop])\n",
    "    mdv = missmean(dopplervel, dims=2)[:]\n",
    "    # interpolate Ur,Vr, heave to the lidar stare grid\n",
    "    ind = findindices( stare_dt[st:en], Vn[\"time\"] )\n",
    "    pitch = indavg( Vn[\"pitch\"], ind) # 11-point mean\n",
    "    roll  = indavg( Vn[\"roll\" ], ind)\n",
    "    heave = indavg( Vn[\"heave\"], ind)\n",
    "    # resync the clock to the VactorNav heave - brittle\n",
    "    stare1dt = stare_dt[st:en] # subset\n",
    "    ind = findindices( stare1dt, Vn[\"time\"] )\n",
    "    heave = Vn[\"heave\"][ind]\n",
    "    shift = get_time_shift(mdv[:],heave[:])\n",
    "    # interpolate for the updated synced time\n",
    "    stare1dt .-= Millisecond((1020-80)*shift)\n",
    "    ind = findindices( stare1dt, Vn[\"time\"] ) # this works\n",
    "    heave = indavg( Vn[\"heave\"], ind)\n",
    "\n",
    "    # mean relative velocity\n",
    "    Ur = zeros(size(dopplervel))\n",
    "    Vr = zeros(size(dopplervel))\n",
    "    ind = findindices( Dates.value.(stare1dt), Dates.value.(UV[\"time\"]))\n",
    "    # result must be 1:1 for stare1dt and ind\n",
    "    ls = length(stare1dt)\n",
    "    li = length(ind)\n",
    "    if li < ls # extend ind with last index of UV\n",
    "        ind = [ind; length(UV[\"time\"]).+zeros(Int32, ls-li)]\n",
    "    end\n",
    "    for ih in 1:ntop # loop to broadcast to consistent size\n",
    "        Ur[:,ih] .= UV[:ur][ind,ih]\n",
    "        Vr[:,ih] .= UV[:vr][ind,ih]\n",
    "    end\n",
    "\n",
    "    # questionable: fill all the mean relative velocities\n",
    "    isf = isfinite.(Vr)\n",
    "    Vr[.!isf] .= mean(Vr[isf])\n",
    "    Ur[.!isf] .= mean(Ur[isf])\n",
    "    \n",
    "    return dopplervel, pitch, roll, heave, Ur, Vr, mdv\n",
    "end\n",
    "\n",
    "function filter_vel_coherent_heave( dopplervel, pitch, roll, heave, mdv )\n",
    "    # get the component of dopplervel coherent with heave,pitch,roll\n",
    "    # allowing for phase shifts\n",
    "    mdv_clean_heave, _ = remove_coherent_component( Float64.(mdv), Float64.(heave) )\n",
    "    mdv_remove = mdv .- mdv_clean_heave # works\n",
    "    return dopplervel .- mdv_remove\n",
    "end\n",
    "\n",
    "# for testing\n",
    "# seconds since dt0\n",
    "\"offset seconds since dt0\"\n",
    "toffs(dt, dt0=DateTime(2024,4,1)) = Millisecond( dt - dt0 ).value / 1000 # seconds\n",
    "\"datetime by adding time in seconds to dt0. Inverse of toffs\"\n",
    "tons(toffs, dt0=DateTime(2024,4,1)) = Millisecond(round(Int64, 1000*toffs)) + dt0\n",
    "\n",
    "#= usage\n",
    "vntoffs = toffs.( Vn[\"time\"] )\n",
    "# test indavg\n",
    "vndt_int = tons.(indavg( vntoffs, ind ))\n",
    "=#\n",
    "\n",
    "function plot_stare_motcor( height, dopplervel, mdv, mdv_remove, pitch, roll, Ur, Vr)\n",
    "    mm = @. minute(stare1dt) + (second(stare1dt) + millisecond(stare1dt)/1000 )/60\n",
    "\n",
    "    clf()\n",
    "    subplot(3,1,1)\n",
    "    pcolormesh(mm, height[1:ntop]/1e3, m2n.(pd(dopplervel.-mdv)), cmap=ColorMap(\"RdYlBu_r\"), vmin=-1, vmax=1)\n",
    "    colorbar()\n",
    "    title(\"vel - mdv\")\n",
    "    ylim([0, 1])\n",
    "\n",
    "    subplot(3,1,2)\n",
    "    pcolormesh(mm, height[1:ntop]/1e3, m2n.(pd(dopplervel.-mdv_remove)), cmap=ColorMap(\"RdYlBu_r\"), vmin=-1, vmax=1)\n",
    "    colorbar()\n",
    "    title(\"vel - mdv coherent with heave\")\n",
    "    ylim([0, 1])\n",
    "\n",
    "    subplot(3,1,3)\n",
    "    w = wtrue.(dopplervel.-heave, Ur, Vr, pitch*pi/180, roll*pi/180)\n",
    "    pcolormesh(mm, height[1:ntop]/1e3, m2n.(pd(w)), cmap=ColorMap(\"RdYlBu_r\"), vmin=-1, vmax=1)\n",
    "    colorbar()\n",
    "    title(\"w heave and tilt angle decompostion\")\n",
    "    ylim([0, 1])\n",
    "\n",
    "    tight_layout()\n",
    "    # Corrections in wtrue (other than adding the -heave) do not seem to be important\n",
    "    # at this time. Motion compensation is probably working, then.\n",
    "    return gcf()\n",
    "end\n",
    "\n",
    "## structure function dissipation\n",
    "\n",
    "# stucture function constants\n",
    "C2ll = 2.0\n",
    "epsilon(A) = sqrt(3/4 * A/C2ll)^3\n",
    "# struf(epsilon, r,r1) = C2ll * epsilon^(2/3) * r^(2/3) * (4 - (r1/r)^2)/3\n",
    "# instruf(w1,w2) = (w1-w2)^2\n",
    "# rho(r1,r) = r^(2/3) * (1 - ((r1/r)^2)/4)\n",
    "# zmid(z1,z2) = (z1 + z2) / 2\n",
    "# plot bin averaged instruf vs rho\n",
    "# fit \n",
    "# D = A*rho + noise\n",
    "# for A and noise\n",
    "# A = 4/3 * C2ll * epsilon^(2/3)\n",
    "\n",
    "\"bin average D2 in equally-populated bins of rho\"\n",
    "function equal_bin(rho, D2; nbin=200, nbin_out_max=17 )\n",
    "    ii = findall(.!ismissing.(rho) .& .!ismissing.(D2) )\n",
    "    nrho = length(ii)\n",
    "    if nrho >= 20\n",
    "        sp = sortperm(rho[ii])\n",
    "        srho = rho[ii][sp]\n",
    "        step = max(1,round(Int32,nrho/nbin))\n",
    "        rhobin = [ 0; rho[ii][sp[step:step:nrho]] ]\n",
    "        jj = findall(.!ismissing.(rhobin) .& isfinite.(rhobin))\n",
    "        D2inbin = binavg(D2[ii], rho[ii], rhobin[jj])\n",
    "        rhoinbin = binavg(rho[ii], rho[ii], rhobin[jj])\n",
    "        nbin_out = min(nbin_out_max, length(rhobin))\n",
    "        return nbin_out, rhobin[1:nbin_out], D2inbin[1:nbin_out], rhoinbin[1:nbin_out]\n",
    "    else\n",
    "        return 1, [missing], [missing], [missing]\n",
    "    end\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "structure function D2, rho, A, epsilon at each level from w stare\n",
    "D2bin, rhobin, A, noise = D2_rho_stare( w, pitch, roll, Ur, Vr; out=17 )\n",
    "\"\"\"\n",
    "function D2_rho_stare( w, pitch, roll, Ur, Vr; nbin_out_max=17 )\n",
    "\n",
    "    nbin_out = nbin_out_max\n",
    "    \n",
    "    (nt, nz) = size(w)\n",
    "    A      = Vector{Union{Missing,Float64}}(missing, nz)\n",
    "    noise  = Vector{Union{Missing,Float64}}(missing, nz)\n",
    "    rhobin = Matrix{Union{Missing,Float64}}(missing, nbin_out, nz)\n",
    "    D2bin  = Matrix{Union{Missing,Float64}}(missing, nbin_out, nz)\n",
    "    for izo in 1:nz # loop vertically\n",
    "        #=\n",
    "        ci1,ci2, li1,li2, it1,iz1,it2,iz2 = lidarindices(nt, nz, izo) # might do outside the loop\n",
    "        zm, dr2, dz2, D2 = displacements( ci1,ci2, Ur*timestep,Vr*timestep,\n",
    "                                          pitch,roll, w; timestep=timestep )\n",
    "        rho = rhopair.(dr2, dz2) # approx r^2/3\n",
    "        # bin average str fcn D2 in equally-populated bins of rho\n",
    "        @show size(rho), size(D2)\n",
    "        rhobin_, D2inbin_, rhoinbin_ = equal_bin(rho, D2)\n",
    "        rhobin[:,izo] .= rhoinbin_[1:nbin_out]\n",
    "        D2bin[ :,izo] .= D2inbin_[ 1:nbin_out]\n",
    "        # regress to get A\n",
    "        ii = .!ismissing.(rhobin[:,izo]) .& .!ismissing.(D2bin[:,izo])\n",
    "        if sum(ii) > 2\n",
    "            A[izo] = anom(rhobin[:,izo][ii]) \\ anom(D2bin[:,izo][ii])\n",
    "            noise[izo] = mean(D2bin[:,izo][ii]) - A[izo] * mean(rhobin[:,izo][ii]) # noise\n",
    "        end\n",
    "        =#\n",
    "        ci1,ci2, li1,li2, it1,iz1,it2,iz2 = lidarindices(nt, nz, izo) # might do outside the loop\n",
    "        zm, dr2, dz2, D2 = displacements( ci1,ci2, Ur*timestep,Vr*timestep,\n",
    "                                          pitch,roll, w; timestep=timestep )\n",
    "        rho = rhopair.(dr2, dz2) # approx r^2/3\n",
    "        # bin average str fcn D2 in equally-populated bins of rho\n",
    "        nbin_actual, rhobin_, D2inbin_, rhoinbin_ = equal_bin(rho, D2; nbin_out_max=nbin_out_max)\n",
    "        rhobin[1:nbin_actual,izo] .= rhoinbin_\n",
    "        D2bin[ 1:nbin_actual,izo] .= D2inbin_\n",
    "        # regress to get A\n",
    "        ii = .!ismissing.(rhobin[1:nbin_actual,izo]) .& .!ismissing.(D2bin[1:nbin_actual,izo])\n",
    "        if sum(ii) > 2\n",
    "            A[izo] = anom(rhobin[1:nbin_actual,izo][ii]) \\ anom(D2bin[1:nbin_actual,izo][ii])\n",
    "            noise[izo] = mean(D2bin[1:nbin_actual,izo][ii]) - A[izo] * mean(rhobin[1:nbin_actual,izo][ii]) # noise\n",
    "        end\n",
    "    end\n",
    "    return D2bin, rhobin, A, noise\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^test D2_rho_stare line by line\n",
    "#=\n",
    "(nt, nz) = size(w)\n",
    "nbin_out_max = 17\n",
    "A      = Vector{Union{Missing,Float64}}(missing, nz)\n",
    "noise  = Vector{Union{Missing,Float64}}(missing, nz)\n",
    "rhobin = Matrix{Union{Missing,Float64}}(missing, nbin_out, nz)\n",
    "D2bin  = Matrix{Union{Missing,Float64}}(missing, nbin_out, nz)\n",
    "\n",
    "izo = 1\n",
    "    ci1,ci2, li1,li2, it1,iz1,it2,iz2 = lidarindices(nt, nz, izo) # might do outside the loop\n",
    "    zm, dr2, dz2, D2 = displacements( ci1,ci2, Ur*timestep,Vr*timestep,\n",
    "                                      pitch*pi/180,roll*pi/180, w; timestep=timestep ) #hacked\n",
    "    rho = rhopair.(dr2, dz2) # approx r^2/3\n",
    "    # bin average str fcn D2 in equally-populated bins of rho\n",
    "    nbin_actual, rhobin_, D2inbin_, rhoinbin_ = equal_bin(rho, D2; nbin_out_max=nbin_out_max)\n",
    "    rhobin[1:nbin_actual,izo] .= rhoinbin_\n",
    "    D2bin[ 1:nbin_actual,izo] .= D2inbin_\n",
    "    # regress to get A\n",
    "    # this is a mess because the bins can be of different sizes, \n",
    "    # resulting in different quality estimates of epsilon\n",
    "    ii = .!ismissing.(rhobin[1:nbin_actual,izo]) .& .!ismissing.(D2bin[1:nbin_actual,izo])\n",
    "    if sum(ii) > 2\n",
    "        A[izo] = anom(rhobin[1:nbin_actual,izo][ii]) \\ anom(D2bin[1:nbin_actual,izo][ii])\n",
    "        noise[izo] = mean(D2bin[1:nbin_actual,izo][ii]) - A[izo] * mean(rhobin[1:nbin_actual,izo][ii]) # noise\n",
    "    end\n",
    "=#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute TKE dissipation in ~10 min chunks by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# line by line test one chunk\n",
    "\n",
    "ntop = 80       # subset vertical levels\n",
    "timestep = 1.02 # s\n",
    "lidarstemdir = \"./data\" # \"/Users/deszoeks/Data/EKAMSAT/lidar\"\n",
    "lidardaydirs = filter( startswith(\"2024\"), readdir(lidarstemdir) )\n",
    "\n",
    "lidardaydir = lidardaydirs[2]\n",
    "dt = Date(lidardaydir, dateformat\"yyyymmdd\")\n",
    "\n",
    "epsi_tmp = Matrix{Union{Missing,Float64}}(missing, 6,ntop)\n",
    "\n",
    "Vn = read_daily_Vn( dt )            # Dict\n",
    "# load daily relative horizontal winds\n",
    "UV = get_daily_meanuv( dt )\n",
    "\n",
    "lidarfile = filter(startswith(\"Stare_116_\"), readdir(joinpath(lidarstemdir,lidardaydir)))[21]\n",
    "splt = split(lidarfile, r\"[_.]\")\n",
    "dt = DateTime(splt[3]*splt[4], dateformat\"yyyymmddHH\") # a DateTime\n",
    "\n",
    "St, _ = read_streamlinexr_stare( dt )\n",
    "height = St[:height][1:ntop]\n",
    "# TO DO: read the next hour to get continuous chunk at end ...\n",
    "st_chunks, en_chunks = read_stare_time( St )\n",
    "ichunk = 5\n",
    "st = st_chunks[ichunk]\n",
    "    en = en_chunks[ichunk]\n",
    "    # read a chunk\n",
    "    dopplervel, pitch, roll, heave, Ur, Vr, mdv = read_stare_chunk( St, Vn, UV, st, en )\n",
    "    # cannot consistently sync heave and dopplervel\n",
    "    #w = filter_vel_coherent_heave( dopplervel, pitch*pi/180, roll*pi/180, heave, mdv )\n",
    "    w = dopplervel .- mdv \n",
    "    # subplot(2,1,1)\n",
    "    # pcolormesh(pd(m2n.(w)), cmap=ColorMap(\"RdYlBu_r\"), vmin=-1, vmax=1)\n",
    "    D2bin, rhobin, A, noise = D2_rho_stare( w, pitch*pi/180, roll*pi/180, Ur, Vr )\n",
    "    epsi_tmp[ichunk,:] = @. epsilon(max(0,A))\n",
    "\n",
    "    subplot(2,1,1)\n",
    "    plot(m2n.(rhobin), m2n.(D2bin), marker=\".\", linewidth=0.4)\n",
    "    xlim([0, 50])\n",
    "    ylim([0, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: compute dissipation for one hour\n",
    "\n",
    "ntop = 80       # subset vertical levels\n",
    "timestep = 1.02 # s\n",
    "lidarstemdir = \"/Users/deszoeks/Data/EKAMSAT/lidar\" # \"./data/lidar\"\n",
    "lidardaydirs = filter( startswith(\"2024\"), readdir(lidarstemdir) )\n",
    "\n",
    "lidardaydir = lidardaydirs[5]\n",
    "dt = Date(lidardaydir, dateformat\"yyyymmdd\")\n",
    "\n",
    "epsi_tmp = Matrix{Union{Missing,Float64}}(missing, 6,ntop)\n",
    "\n",
    "Vn = read_daily_Vn( dt )            # Dict\n",
    "# load daily relative horizontal winds\n",
    "UV = get_daily_meanuv( dt )\n",
    "\n",
    "lidarfile = filter(startswith(\"Stare_116_\"), readdir(joinpath(lidarstemdir,lidardaydir)))[12]\n",
    "dt = Date(lidardaydir, dateformat\"yyyymmdd\")\n",
    "epsi_tmp = Matrix{Union{Missing,Float64}}(missing, 6,ntop)\n",
    "\n",
    "Vn = read_daily_Vn( dt )            # Dict\n",
    "# load daily relative horizontal winds\n",
    "UV = get_daily_meanuv( dt )\n",
    "\n",
    "lidarfile = filter(startswith(\"Stare_116_\"), readdir(joinpath(lidarstemdir,lidardaydir)))[12]\n",
    "splt = split(lidarfile, r\"[_.]\")\n",
    "dt = DateTime(splt[3]*splt[4], dateformat\"yyyymmddHH\") # a DateTime\n",
    "\n",
    "St, _ = read_streamlinexr_stare( dt )\n",
    "height = St[:height][1:ntop]\n",
    "# TO DO: read the next hour to get continuous chunk at end ...\n",
    "st_chunks, en_chunks = read_stare_time( St )\n",
    "for (ichunk, st) in enumerate(st_chunks)\n",
    "    en = en_chunks[ichunk]\n",
    "    # read a chunk\n",
    "    dopplervel, pitch, roll, heave, Ur, Vr, mdv = read_stare_chunk( St, Vn, UV, st, en )\n",
    "    if any(isfinite.(Ur)) && any(isfinite.(Vr))\n",
    "        # cannot consistently sync heave and dopplervel\n",
    "        #w = filter_vel_coherent_heave( dopplervel, pitch*pi/180, roll*pi/180, heave, mdv )\n",
    "        w = dopplervel .- mdv \n",
    "        D2bin, rhobin, A, noise = D2_rho_stare( w, pitch*pi/180, roll*pi/180, Ur, Vr )\n",
    "        epsi_tmp[ichunk,:] = @. epsilon(max(0,A))\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42-element Vector{String}:\n",
       " \"20240428\"\n",
       " \"20240429\"\n",
       " \"20240430\"\n",
       " \"20240501\"\n",
       " \"20240502\"\n",
       " \"20240503\"\n",
       " \"20240504\"\n",
       " \"20240505\"\n",
       " \"20240506\"\n",
       " \"20240507\"\n",
       " ⋮\n",
       " \"20240605\"\n",
       " \"20240606\"\n",
       " \"20240607\"\n",
       " \"20240608\"\n",
       " \"20240609\"\n",
       " \"20240610\"\n",
       " \"20240611\"\n",
       " \"20240612\"\n",
       " \"20240613\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lidarstemdir = \"/Users/deszoeks/Data/EKAMSAT/lidar\" # \"./data/lidar\"\n",
    "lidardaydirs = filter( startswith(\"2024\"), readdir(lidarstemdir) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8640-element Vector{Float64}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " ⋮\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loop through lidar data and compute TKE dissipation rate\n",
    "\n",
    "ntop = 80       # subset vertical levels\n",
    "timestep = 1.02 # s\n",
    "lidarstemdir = \"/Users/deszoeks/Data/EKAMSAT/lidar\" # \"./data/lidar\"\n",
    "lidardaydirs = filter( startswith(\"2024\"), readdir(lidarstemdir) )\n",
    "epsi = Matrix{Union{Missing,Float64}}(missing, 6*24*60, ntop)\n",
    "lidardtstart = zeros(6*24*60)\n",
    "lidardtend = zeros(6*24*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for lidardaydir in lidardaydirs[2:16] # files available leg 1\n",
    "for lidardaydir in lidardaydirs[2:6] # files available leg 1\n",
    "# for lidardaydir in lidardaydirs[17:end] # files available leg 2\n",
    "    dt = Date(lidardaydir, dateformat\"yyyymmdd\") # a Date\n",
    "    print(\"$(lidardaydir) \")\n",
    "    # can't sync motion with clock!\n",
    "    try # load daily vectornav  \n",
    "        Vn = read_daily_Vn( dt )            # Dict\n",
    "    catch\n",
    "        print(\"no VectorNav for $(dt)\")\n",
    "    end\n",
    "    try # load daily relative horizontal winds\n",
    "        UV = get_daily_meanuv( dt )         # NCDataset\n",
    "    catch\n",
    "        print(\"no mean wind for $(dt)\\n\")\n",
    "        #save epsilon.jld2 just in case\n",
    "        # jldopen(\"epsilon_tmp.jld2\", \"w+\") do file\n",
    "        #     file[\"epsilon\"] = epsi\n",
    "    end\n",
    "\n",
    "    # load hourly lidar stares\n",
    "    bigind = 0 # index time, save daily\n",
    "    for lidarfile in filter(startswith(\"Stare_116_\"), readdir(joinpath(lidarstemdir,lidardaydir)))\n",
    "        splt = split(lidarfile, r\"[_.]\")\n",
    "        dt = DateTime(splt[3]*splt[4], dateformat\"yyyymmddHH\") # a DateTime\n",
    "        print(\"$(splt[4]) \")\n",
    "        \n",
    "        St, _ = read_streamlinexr_stare( dt )\n",
    "        height = St[:height][1:ntop]\n",
    "        # TO DO: read the next hour to get continuous chunk at end ...\n",
    "        st_chunks, en_chunks = read_stare_time( St )\n",
    "        stare_dt = @. ( dt \n",
    "            + Millisecond(round(Int64, St[:time] * 3_600_000 )) \n",
    "            - lidar_clock_fast_by )\n",
    "        for (ichunk, st) in enumerate(st_chunks)\n",
    "            en = en_chunks[ichunk]\n",
    "            bigind += 1\n",
    "            # read a chunk\n",
    "            dopplervel, pitch, roll, heave, Ur, Vr, mdv = read_stare_chunk( St, Vn, UV, st, en )\n",
    "            if any(isfinite.(Ur)) && any(isfinite.(Vr))\n",
    "                # cannot consistently sync heave and dopplervel\n",
    "                #w = filter_vel_coherent_heave( dopplervel, pitch*pi/180, roll*pi/180, heave, mdv )\n",
    "                w = dopplervel .- mdv \n",
    "                D2bin, rhobin, A, noise = D2_rho_stare( w, pitch*pi/180, roll*pi/180, Ur, Vr )\n",
    "                epsi[bigind,:] .= @. epsilon.(max(0,A))\n",
    "            else\n",
    "                epsi[bigind,:] .= -4 # code for missing wind\n",
    "            end\n",
    "            lidardtstart[bigind] = stare_dt[st]\n",
    "            lidardtend[bigind] = stare_dt[en]\n",
    "        end\n",
    "    end\n",
    "    print(\"\\n\")\n",
    "    #save data daily to epsilon.jld2\n",
    "    fileout = \"epsilon_$(Dates.format(dt, dateformat\"yyyymmdd\")).jld2\"\n",
    "    print(\"Saving $(fileout)\\n\")\n",
    "    jldopen(fileout, \"w+\") do file\n",
    "        file[\"epsilon\"] = epsi[1:bigind,:]\n",
    "        file[\"start_dt\"] = lidardtstart[1:bigind]\n",
    "        file[\"end_dt\"]   = lidardtend[1:bigind]\n",
    "    end\n",
    "end\n",
    "\n",
    "# notes\n",
    "# heave channel constant on 2024-5-12\n",
    "# no mean wind on 2024-05-12\n",
    "# 20240519 no VectorNav for 2024-05-19 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240429 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n",
      "Saving staredt_20240429.jld2\n",
      "20240430 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n",
      "Saving staredt_20240430.jld2\n",
      "20240501 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n",
      "Saving staredt_20240501.jld2\n",
      "20240502 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n",
      "Saving staredt_20240502.jld2\n",
      "20240503 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n",
      "Saving staredt_20240503.jld2\n",
      "20240504 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n",
      "Saving staredt_20240504.jld2\n",
      "20240505 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n",
      "Saving staredt_20240505.jld2\n",
      "20240506 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n",
      "Saving staredt_20240506.jld2\n",
      "20240507 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n",
      "Saving staredt_20240507.jld2\n",
      "20240508 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n",
      "Saving staredt_20240508.jld2\n",
      "20240509 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n",
      "Saving staredt_20240509.jld2\n",
      "20240510 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n",
      "Saving staredt_20240510.jld2\n",
      "20240511 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n",
      "Saving staredt_20240511.jld2\n",
      "20240512 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n",
      "Saving staredt_20240512.jld2\n",
      "20240513 no mean wind for 2024-05-13\n",
      "00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n",
      "Saving staredt_20240513.jld2\n"
     ]
    }
   ],
   "source": [
    "# just compute and save start and end times for stares\n",
    "\n",
    "lidar_clock_fast_by = Millisecond( 126832 ) # ~2 minutes; check this!\n",
    "\n",
    "lidardtstart = Vector{DateTime}(undef, 6*24*60)\n",
    "lidardtend = Vector{DateTime}(undef, 6*24*60)\n",
    "\n",
    "for lidardaydir in lidardaydirs[2:16] # files available leg 1\n",
    "    # for lidardaydir in lidardaydirs[17:end] # files available leg 2\n",
    "    dt = Date(lidardaydir, dateformat\"yyyymmdd\") # a Date\n",
    "    print(\"$(lidardaydir) \")\n",
    "    # can't sync motion with clock!\n",
    "    try # load daily vectornav  \n",
    "        Vn = read_daily_Vn(dt)            # Dict\n",
    "    catch\n",
    "        print(\"no VectorNav for $(dt)\")\n",
    "    end\n",
    "    try # load daily relative horizontal winds\n",
    "        UV = get_daily_meanuv(dt)         # NCDataset\n",
    "    catch\n",
    "        print(\"no mean wind for $(dt)\\n\")\n",
    "        #save epsilon.jld2 just in case\n",
    "        # jldopen(\"epsilon_tmp.jld2\", \"w+\") do file\n",
    "        #     file[\"epsilon\"] = epsi\n",
    "    end\n",
    "\n",
    "    # load hourly lidar stares\n",
    "    bigind = 0 # index time, save daily and reset bigind daily\n",
    "    for lidarfile in filter(startswith(\"Stare_116_\"), readdir(joinpath(lidarstemdir, lidardaydir)))\n",
    "        splt = split(lidarfile, r\"[_.]\")\n",
    "        dt = DateTime(splt[3] * splt[4], dateformat\"yyyymmddHH\") # a DateTime\n",
    "        print(\"$(splt[4]) \")\n",
    "\n",
    "        St, _ = read_streamlinexr_stare(dt)\n",
    "\n",
    "        # TO DO: read the next hour to get continuous chunk at end ...\n",
    "        st_chunks, en_chunks = read_stare_time(St)\n",
    "        stare_dt = @. ( dt \n",
    "            + Millisecond(round(Int64, St[:time] * 3_600_000 )) \n",
    "            - lidar_clock_fast_by )\n",
    "        for (ichunk, st) in enumerate(st_chunks)\n",
    "            en = en_chunks[ichunk]\n",
    "            bigind += 1\n",
    "            lidardtstart[bigind] = stare_dt[st]\n",
    "            lidardtend[bigind]   = stare_dt[en]\n",
    "            # epsi[bigind,:] .= -4 # code for missing wind\n",
    "        end\n",
    "    end\n",
    "    print(\"\\n\")\n",
    "    #save data daily to epsilon.jld2\n",
    "    fileout = \"staredt_$(Dates.format(dt, dateformat\"yyyymmdd\")).jld2\"\n",
    "    print(\"Saving $(fileout)\\n\")\n",
    "    jldopen(fileout, \"a+\") do file\n",
    "        file[\"start_dt\"] = lidardtstart[1:bigind]\n",
    "        file[\"end_dt\"]   = lidardtend[1:bigind]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <matplotlib.collections.QuadMesh object at 0x37383a7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jldopen(\"./epsilon_data/epsilon_tmp.jld2\", \"r\") do file\n",
    "    epsi = file[\"epsilon\"]\n",
    "end\n",
    "\n",
    "pcolormesh(pd(m2n.(epsi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `stare1dt` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `stare1dt` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Projects/ASTRAL/lidar/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X30sZmlsZQ==.jl:25"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "\n",
    "# check that St and Vn data are synced\n",
    "clf()\n",
    "subplot(2,1,1)\n",
    "plot(hp(mdv))\n",
    "plot(hp(heave))\n",
    "xlim([400, 440])\n",
    "# check lags\n",
    "xc = xcorr( hp(mdv[:]), hp(heave) )\n",
    "subplot(2,1,2)\n",
    "plot( -(length(mdv)-1):length(mdv)-1, xc )\n",
    "xlim([-40, 40])\n",
    "#length(xc), 2*(length(mdv)-1) + 1\n",
    "argmax(xc) - length(mdv) # 0-lag center moved to 0\n",
    "# length(xc), length(mdv), 2*length(heave)\n",
    "\n",
    "\n",
    "plot(mdv)\n",
    "# plot(mdv_clean_heave)\n",
    "# plot(mdv_clean_heavepitch)\n",
    "# plot(mdv_clean_heavepitch)\n",
    "\n",
    "\n",
    "plot(stare1dt, heave)\n",
    "plot(stare1dt, mdv)\n",
    "plot(stare1dt, mdv - heave) # subtract heave because its downward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare the structure function analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `clf` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `clf` not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[1]:1"
     ]
    }
   ],
   "source": [
    "clf()\n",
    "subplot(2,1,1)\n",
    "plot(rhobin, D2bin, marker=\"*\")\n",
    "plot([0, 2.2*mean(rhobin[1:17])], noise .+ A.*[0, 2.2*mean(rhobin[1:17])])\n",
    "xlim([0, 30])\n",
    "xlabel(L\"\\rho = r^{2/3}\"\n",
    "title(\"TKE dissipation = 1.4 $\\times 10^{-4}$\")\n",
    "epsilon(A) # 1.3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
