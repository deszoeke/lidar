{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using Pkg; Pkg.activate(\".\")\n",
    "\n",
    "using Dates\n",
    "using Statistics\n",
    "using Interpolations\n",
    "using DSP\n",
    "using FFTW\n",
    "using NCDatasets\n",
    "using JLD2\n",
    "using Printf\n",
    "\n",
    "# include(\"./readers.jl\")\n",
    "include(\"./read_lidar.jl\")\n",
    "using .read_lidar\n",
    "# using MAT\n",
    "\n",
    "using PyPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "pd = permutedims\n",
    "m2n(x) = ismissing(x) ? NaN : x\n",
    "\n",
    "\"bin average y(x) in bins b of coordinate x\"\n",
    "function binavg(y, x, b)\n",
    "    a = zeros(length(b))\n",
    "    c = zeros(length(b))\n",
    "    for (i,x) in enumerate(x)\n",
    "        bi = findlast(j -> j < x, b)\n",
    "        a[bi] += y[i]\n",
    "        c[bi] += 1\n",
    "    end\n",
    "    return a./c\n",
    "end\n",
    "\n",
    "# functions for masking and averaging data\n",
    "\n",
    "\"NaN -> missing\"\n",
    "n2m(x) = isfinite.(x) ? x : missing\n",
    "\n",
    "\"result is x; set to missing iff i<thr\"\n",
    "masklowi(x, i, thr=1.03) = i<thr ? missing : x\n",
    "\n",
    "\"mean along dimension dims, skipping missing\"\n",
    "missmean(X; dims=1) = mapslices(x -> mean(skipmissing(x)), X, dims=dims)\n",
    "\n",
    "\"anomaly\"\n",
    "anom(x; dims=1) = x.-mean(x; dims=dims)\n",
    "\n",
    "# highpass filter\n",
    "\"\"\"\n",
    "hp(x, fcutoff=1/80)    highpass filter x,\n",
    "by default filtfilt 4th-order Butterworth, fs=1\n",
    "\"\"\"\n",
    "function hp(x, fcutoff=1/80;\n",
    "    order=4,\n",
    "    designmethod=Butterworth(order), \n",
    "    fs=1,\n",
    "    responsetype = Highpass(fcutoff; fs=fs) )\n",
    "    \n",
    "    filtfilt(digitalfilter(responsetype, designmethod), x)\n",
    "end\n",
    "\n",
    "\n",
    "# make simple linear temporal interpolation\n",
    "# maybe fast\n",
    "# most time is spent searching for indices\n",
    "# indices are monotonic\n",
    "\n",
    "\"return all the indices i such that each xl[i] is the first >= each xs.\"\n",
    "# untested suggestion:\n",
    "# findindices(xs, xl) = [ findfirst(xl .>= x) for x in xs ]\n",
    "function findindices(xs, xl)\n",
    "    # xs needles define quarries in haystack xl\n",
    "    xs = filter(x -> x<=last(xl), xs) # prefilter to avoid running off the end of xl\n",
    "    ind = zeros(Int64, size(xs))\n",
    "    i = 1\n",
    "    for (j,x) in enumerate(xs)\n",
    "        while xl[i] < x\n",
    "            i += 1\n",
    "        end\n",
    "        ind[j] = i\n",
    "    end\n",
    "    return ind\n",
    "end\n",
    "\n",
    "\n",
    "# # remove old method for centered average - SPd 2024-11-11\n",
    "# \"average xl centered within +-half points of the index of xl\"\n",
    "# function indavg(xl, ind; half=10)\n",
    "#     xm = zeros(Float64, size(ind))\n",
    "#     for (i,idx) in enumerate(ind)\n",
    "#         ii = max(1,idx-half) : min(length(xl),idx+half)\n",
    "#         xm[i] = sum(Float64.(xl[ii])) / (2*half+1)\n",
    "#     end\n",
    "#     return xm\n",
    "# end\n",
    "# # replace with average to right of ind\n",
    "\"average xl within windows to right of points of the index ind of xl\"\n",
    "function indavg(xl, ind; full=20)\n",
    "    xm = zeros(Float64, size(ind))\n",
    "    for (i,idx) in enumerate(ind)\n",
    "        ii = max(1,idx) : min(length(xl),idx+full)\n",
    "        # xm[i] = sum(Float64.(xl[ii])) / (full+1)\n",
    "        xm[i] = mean(Float64.(xl[ii]))\n",
    "    end\n",
    "    return xm\n",
    "end\n",
    "\n",
    "# test data (precompiles)\n",
    "let xl = 1:60_000_000, xs = 20:20:60_000_000\n",
    "    ind = findindices(xs, xl)\n",
    "    indavg(xl, ind)\n",
    "end\n",
    "\n",
    "# Adjust true vertical velocity for relative wind * sin(tilt)\n",
    "# and the platform velocity\n",
    "trigs(pitch, roll) = ( cos(pitch), sin(pitch), cos(roll), sin(roll) )\n",
    "# cospitch, sinpitch, cosroll, sinroll = trigs(pitch, roll)\n",
    "\n",
    "function wtrue(w, Ur, Vr, pitch, roll)\n",
    "    cospitch, sinpitch, cosroll, sinroll = trigs(pitch, roll)\n",
    "    wtrue = ( w + Ur*sinpitch - Vr*cospitch*sinroll ) / (cospitch*cosroll)\n",
    "end\n",
    "\n",
    "# displacements with no adjustment for tilting into the horizontal wind \n",
    "# U, V vary slowly; pitch,roll,w vary fast\n",
    "# there are nt*(nt-1)/2 ~ O(nt^2) outputs, so correct stuff first\n",
    "\n",
    "## don't use:\n",
    "# \"return the coherent component of signal1 and signal2\"\n",
    "# function coherent_component(signal1::Vector{Float64}, signal2::Vector{Float64})\n",
    "#     # Fourier Transform of the signals\n",
    "#     S1 = fft(signal1)\n",
    "#     S2 = fft(signal2)\n",
    "    \n",
    "#     # Compute cross-spectral density\n",
    "#     P12 = S1 .* conj(S2)\n",
    "#     # P21 = conj(P12)\n",
    "    \n",
    "#     # Compute auto-spectral density\n",
    "#     P11 = S1 .* conj(S1)\n",
    "#     P22 = S2 .* conj(S2)\n",
    "    \n",
    "#     # Compute coherence\n",
    "#     coherence = abs.(P12).^2 ./ (P11 .* P22)\n",
    "    \n",
    "#     # Compute the coherent part\n",
    "#     coherent_part_S1 = coherence .* S2\n",
    "#     coherent_part_S2 = coherence .* S1\n",
    "    \n",
    "#     # Inverse Fourier Transform to get the time-domain signals\n",
    "#     coherent_signal1 = real(ifft(coherent_part_S1))\n",
    "#     coherent_signal2 = real(ifft(coherent_part_S2))\n",
    "    \n",
    "#     return coherent_signal1, coherent_signal2\n",
    "# end\n",
    "\n",
    "# \"remove the coherent component of signal1 and signal2\"\n",
    "# function remove_coherent_component(signal1::Vector{Float64}, signal2::Vector{Float64})\n",
    "#     # Fourier Transform of the signals\n",
    "#     S1 = fft(signal1)\n",
    "#     S2 = fft(signal2)\n",
    "    \n",
    "#     # Compute cross-spectral density\n",
    "#     P12 = S1 .* conj(S2)\n",
    "#     # P21 = conj(P12)\n",
    "    \n",
    "#     # Compute auto-spectral density\n",
    "#     P11 = S1 .* conj(S1)\n",
    "#     P22 = S2 .* conj(S2)\n",
    "    \n",
    "#     # Compute coherence\n",
    "#     coherence = abs.(P12).^2 ./ (P11 .* P22)\n",
    "    \n",
    "#     # Compute the coherent part\n",
    "#     coherent_part_S1 = coherence .* S2\n",
    "#     coherent_part_S2 = coherence .* S1\n",
    "    \n",
    "#     # Remove the coherent part\n",
    "#     clean_S1 = S1 .- coherent_part_S1\n",
    "#     clean_S2 = S2 .- coherent_part_S2\n",
    "    \n",
    "#     # Inverse Fourier Transform to get the time-domain signals\n",
    "#     clean_signal1 = real(ifft(clean_S1))\n",
    "#     clean_signal2 = real(ifft(clean_S2))\n",
    "    \n",
    "#     return clean_signal1, clean_signal2\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CartesianIndex{2}[CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1), CartesianIndex(1, 1)  …  CartesianIndex(996, 1), CartesianIndex(996, 1), CartesianIndex(996, 1), CartesianIndex(996, 1), CartesianIndex(997, 1), CartesianIndex(997, 1), CartesianIndex(997, 1), CartesianIndex(998, 1), CartesianIndex(998, 1), CartesianIndex(999, 1)], CartesianIndex{2}[CartesianIndex(2, 1), CartesianIndex(3, 1), CartesianIndex(4, 1), CartesianIndex(5, 1), CartesianIndex(6, 1), CartesianIndex(7, 1), CartesianIndex(8, 1), CartesianIndex(9, 1), CartesianIndex(10, 1), CartesianIndex(11, 1)  …  CartesianIndex(997, 1), CartesianIndex(998, 1), CartesianIndex(999, 1), CartesianIndex(1000, 1), CartesianIndex(998, 1), CartesianIndex(999, 1), CartesianIndex(1000, 1), CartesianIndex(999, 1), CartesianIndex(1000, 1), CartesianIndex(1000, 1)], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  499491, 499492, 499493, 499494, 499495, 499496, 499497, 499498, 499499, 499500], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  499491, 499492, 499493, 499494, 499495, 499496, 499497, 499498, 499499, 499500], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  996, 996, 996, 996, 997, 997, 997, 998, 998, 999], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11  …  997, 998, 999, 1000, 998, 999, 1000, 999, 1000, 1000], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# functions for structure functions\n",
    "\n",
    "# generate unique pairs of indices\n",
    "\"index unique pairs in a vector of length n\"\n",
    "function uniquepairs(n) \n",
    "    [ [l1, l2] for l1 in 1:n for l2 in (l1+1):n ]\n",
    "end\n",
    "\"index pairs of points in adjacent levels\"\n",
    "allcross(n) = [ [l1, l2] for l1 in 1:n for l2 in 1:n ]\n",
    "\n",
    "# beam geometry\n",
    "\"lidar beam range\"\n",
    "rng(iz, rangegate=24.0) = rangegate * (iz-1 + 0.5)\n",
    "\n",
    "\"\"\"\n",
    "compile indices of lidar volumes to be compared with\n",
    "structure functions\n",
    "\"\"\"\n",
    "function lidarindices(nt, nz, z1=1; nlevelstats=1)\n",
    "    if nlevelstats == 3\n",
    "        # The complete set that doesn't repeat pairs is \n",
    "        # 1 the complete set of nt*(n-1)/2 pairs for the top level (3)\n",
    "        # 2 the 2*nt*nt sets of pairs between every point in top (3) level and the next 2 levels\n",
    "        # Iteratively slide this box upward by 1 level for each level.\n",
    "    \n",
    "        # index pairs in middle level 2-2\n",
    "        up = uniquepairs(nt)\n",
    "        it1 = map(i->i[1], up) # time indices for pairs of point1, point2\n",
    "        it2 = map(i->i[2], up)\n",
    "        ci1_r22 = CartesianIndex.(tuple.(it1,z1)) # 1st point in pair lev\n",
    "        ci2_r22 = CartesianIndex.(tuple.(it2,z1)) # 2nd \n",
    "    \n",
    "        # index pairs of points from level 2-1, and 2-3\n",
    "        ac = allcross(nt)\n",
    "        it1 = map(i->i[1], ac)\n",
    "        it2 = map(i->i[2], ac)\n",
    "        ci1_r21 = ci1_r23 = CartesianIndex.(tuple.(it1,2))\n",
    "        ci2_r21 = CartesianIndex.(tuple.(it2,z1-1))\n",
    "        ci2_r23 = CartesianIndex.(tuple.(it2,z1+1))\n",
    "    \n",
    "        # omnibus set of cartesian index pairs for a level, including points in lev above and below\n",
    "        ci1 = [ci1_r23; ci1_r22; ci1_r21] # first of pairs\n",
    "        ci2 = [ci2_r23; ci2_r22; ci2_r21]\n",
    "        li1 = LinearIndices(ci1)\n",
    "        li2 = LinearIndices(ci2)\n",
    "        \n",
    "    elseif nlevelstats == 1\n",
    "        # just use structure function velocity pairs from one level of lidar range\n",
    "        up = uniquepairs(nt)\n",
    "        it1 = map(i->i[1], up) # time indices for pairs of point1, point2\n",
    "        it2 = map(i->i[2], up)\n",
    "        ci1_r11 = CartesianIndex.(tuple.(it1,z1)) # 1st point in pair lev\n",
    "        ci2_r11 = CartesianIndex.(tuple.(it2,z1)) # 2nd point in same lev\n",
    "    \n",
    "        # set of cartesian index pairs for a level, including points in lev above and below\n",
    "        ci1 = ci1_r11 # first of pairs\n",
    "        ci2 = ci2_r11\n",
    "        li1 = LinearIndices(ci1)\n",
    "        li2 = LinearIndices(ci2)\n",
    "    end\n",
    "    \n",
    "    it1 = map(idx->idx[1], ci1) #  t index of first point(s)\n",
    "    iz1 = map(idx->idx[2], ci1) #  z index of first\n",
    "    it2 = map(idx->idx[1], ci2) #  t       of second points(s)\n",
    "    iz2 = map(idx->idx[2], ci2) #  z          second\n",
    "    \n",
    "    return ci1,ci2, li1,li2, it1,iz1,it2,iz2\n",
    "end\n",
    "\n",
    "# try example\n",
    "ci1,ci2, li1,li2, it1,iz1,it2,iz2 = lidarindices(1000, 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rhopair"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displacments and structure functions \n",
    "\n",
    "rangegate = 24.0 # for ASTRAL 2024 Halo Photonics StreamLineXR\n",
    "\n",
    "\"\"\"\n",
    "zm, dr2, dz2, D2 = displacements( ci1,ci2, Udt,Vdt, pitch,roll, w; rangegate=rangegate)\n",
    "Displacements of sample pairs for one (vertical) subvolume.\n",
    "\"\"\"\n",
    "function displacements( ci1,ci2, Udt,Vdt, pitch,roll, w; rangegate=rangegate , timestep=timestep)\n",
    "    # get the individual indices\n",
    "    it1 = map(idx->idx[1], ci1) #  t index of first point(s)\n",
    "    iz1 = map(idx->idx[2], ci1) #  z index of first\n",
    "    it2 = map(idx->idx[1], ci2) #  t       of second points(s)\n",
    "    iz2 = map(idx->idx[2], ci2) #  z          second\n",
    "\n",
    "    rng(iz) = rangegate * (iz-1 + 0.5) # center of gates\n",
    "\n",
    "    # horiz translation of the sample volumes by mean wind\n",
    "    Udtbar = @. (Udt[iz2] + Udt[iz1]) / 2\n",
    "    Vdtbar = @. (Vdt[iz2] + Vdt[iz1]) / 2\n",
    "    X = @. Udtbar * (it2 - it1)\n",
    "    Y = @. Vdtbar * (it2 - it1)\n",
    "    # vertical middle of pair\n",
    "    zm = @. (rng(iz2) * cos(pitch[it2])*cos(roll[it2]) + rng(iz1) * cos(pitch[it1])*cos(roll[it1])) / 2\n",
    "    # displacement between pair of points\n",
    "    dz = @.     rng(iz2) * cos(pitch[it2])*cos(roll[it2]) - rng(iz1) * cos(pitch[it1])*cos(roll[it1])\n",
    "    dx = @. X + rng(iz2) *-sin(pitch[it2])                - rng(iz1) *-sin(pitch[it1])\n",
    "    dy = @. Y + rng(iz2) * cos(pitch[it2])*sin(roll[it2]) - rng(iz1) * cos(pitch[it1])*sin(roll[it1])\n",
    "    # distance between\n",
    "    dz2 = dz .* dz\n",
    "    dr2 = @. dz2 + dx*dx + dy*dy\n",
    "    # CORRECT W for HEAVE and for TILTING into the horizontal wind\n",
    "    # vel structure function\n",
    "    D2 = @. (w[ci2] - w[ci1])^2\n",
    "    # return properties of pairs\n",
    "    return zm, dr2, dz2, D2\n",
    "end\n",
    "\n",
    "\"dr^2/3 (1-(dz/dr)^2/4) displacement function for computing dissipation from structure function pairs\"\n",
    "rhopair(dr2, dz2) = dr2^(1/3) * (1 - dz2/(4*dr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moved to read_lidar.jl: utilities for requesting, reading, and subsetting data from files \n",
    "\n",
    "#=\n",
    "# stare functions to find and use start and end indices of stare chunks\n",
    "\n",
    "module stare\n",
    "\n",
    "# used in read_lidar_chunks and tests\n",
    "export hour_beams\n",
    "export all_gaps\n",
    "export all_start_end_indices\n",
    "export query_start_end_chunks\n",
    "export all_chunks\n",
    "\n",
    "# Request chunk subscripts and start and end times in any time window.\n",
    "\"return time as a decimal hour from the start time\"\n",
    "function hour_beams(time, ibeam1, start_time)\n",
    "    # time is decimal hour of the day\n",
    "    # ibeam1 is the index of time corresponding to start_time\n",
    "    # start_time is the DateTime when a new file starts\n",
    "    \n",
    "    # expand time axis to include day and decimal hour\n",
    "    ibeam_expand = [ findlast( ibeam1 .<= i ) for i in eachindex(time) ]\n",
    "    dtbeams = @. floor(start_time, Day)[ibeam_expand] \n",
    "    hourbeams = @. 24*Dates.value(Day(dtbeams-dtbeams[1])) + time\n",
    "end\n",
    "\"hour_beams(timeangles::Dict)\"\n",
    "hour_beams(ta::Dict) = hour_beams(ta[:time], ta[:ibeam1], ta[:start_time])\n",
    "\n",
    "# identify indices of chunks from gaps\n",
    "function all_gaps(t)\n",
    "    ien = findall( diff(t) .> 0.01 ) # gap of more than a few tens of seconds\n",
    "    ist = ien .+ 1 # start of the next chunk\n",
    "    return ien, ist\n",
    "end\n",
    "\n",
    "function all_start_end_indices(ien, ist)\n",
    "    # ii increasing-order vector of all start and end indices\n",
    "    ii = sort( permutedims([ien ist])[:] ) # ordered; this should sort them, but sort() for good measure\n",
    "    jj = LinearIndices(ii) # jj indexes ii\n",
    "    # ii alternates end start-end ... start ii[jjen] == ien; ii[jjst] == ist\n",
    "    jjen = jj[1:2:end]\n",
    "    jjst = jj[2:2:end] # even starts of next chunks follow previous odd ends\n",
    "    # NOTE: a view can change the parity of starts and ends\n",
    "    return ii,jj, jjen,jjst\n",
    "end\n",
    "# ii is the big array of start and end indices\n",
    "\n",
    "# functions to check if an index i is an end point; or a start point\n",
    "isend(i) = in(i, Ref(ien)) # Boolean true for interval chunk end points of ii\n",
    "isstart(i) = in(i, Ref(ist)) # Boolean true for interval chunk start points of ii\n",
    "\n",
    "# NOTE ii default is computed by all_start_end_indices, but not available in the lexical scope\n",
    "# so ii MUST be provided in the call\n",
    "\n",
    "# end index corresponding to a start indes\n",
    "# get the index j such that i = ii[j]\n",
    "thisj(i, ii) = findfirst(ii.==i)\n",
    "# get the end index ien corresponding to the start index ist of the same chunk\n",
    "chunken(ist, ii) = ii[thisj(ist, ii) + 1]\n",
    "# get the start index ist corresp to the end index ien of the same chunk\n",
    "chunkst(ien, ii) = ii[thisj(ien, ii) - 1]\n",
    "\n",
    "# index the next chunk\n",
    "nextchunki(i, ii) = ii[thisj(i, ii) + 2]\n",
    "# index the previous chunk\n",
    "prevchunki(i, ii) = ii[thisj(i, ii) - 2]\n",
    "\n",
    "# Find chunks within query start and end times.\n",
    "\"\"\"\n",
    "Request start and end chunks based on time indices.\n",
    "Inclusive: include chunks for which query falls in the middle.\n",
    "\"\"\"\n",
    "function query_start_end_chunks(qst, qen; ien=ien, ist=ist)\n",
    "    ien1 = ien[findfirst(x-> x>=qst , ien)] # end index of first queried chunk     # do in this order\n",
    "    ist1 = ist[findlast( x-> x <ien1, ist)] # start index of first queried chunk\n",
    "    lastist = ist[findlast( x-> x<=qen   , ist)] # start index of last queried chunk\n",
    "    lastien = ien[findfirst(x-> x>lastist, ien)] # end index of last queried chunk\n",
    "    ist1,ien1, lastist,lastien\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "request all start indices and all end indices between \n",
    "index of start of first chunk and end of last chunk lastien\n",
    "\"\"\"\n",
    "function all_chunks(ist1, lastien, ii)\n",
    "    # pickets of queried subset\n",
    "    iisub = ii[ findall(x-> ist1<=x<=lastien, ii) ] # start-end ... start-end ; no orphans\n",
    "\n",
    "    istsub = iisub[1:2:end] # iisub parity switched from ii\n",
    "    iensub = iisub[2:2:end] # even ends follow odd starts of same chunk\n",
    "    return istsub, iensub # start-end pairs\n",
    "end\n",
    "\n",
    "end # module stare\n",
    "\n",
    "using .stare\n",
    "=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "read_lidar_chunks"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## utility functions to determine which file a chunk in and its indices in that file\n",
    "\n",
    "# !MOVE utilities for requesting, reading, and subsetting data from files\n",
    "# TO read_lidar.jl\n",
    "# include(\"read_lidar.jl\")\n",
    "\n",
    "# internal functions:\n",
    "# get_stare_files, thisfile_idx, idx_beams_in_file, thisfile_start_end, thisfile_name\n",
    "# export read_lidar_chunks # used\n",
    "# export get_all_file_start_end_idxs # exported to test\n",
    "\n",
    "# time index hierarchy:\n",
    "#     beam\n",
    "#   stare chunk [start, end]\n",
    "# file [start, end]\n",
    "\n",
    "# Collect data start indices times of all files in ibeam1\n",
    "# number of beams in each file nbeam\n",
    "# so last beam index in a file is ibeam1+nbeam-1\n",
    "# get_start_fileidx(tidx) = findlast(t-> ibeam1<=t, tidx)\n",
    "# get_end_fileidx(tidx) = findfirst(t-> ibeam1+nbeam-1>=t, tidx)\n",
    "get_stare_files(tidx, files) = files[get_fileidx(tidx)]\n",
    "\n",
    "# vectors of all file start and end indices\n",
    "function get_all_file_start_end_idxs(ta::Dict)\n",
    "    fi1 = ta[:ibeam1][:]\n",
    "    filast = @. fi1 + ta[:nbeams] - 1\n",
    "    return fi1, filast\n",
    "end\n",
    "function get_all_file_start_end_idxs(file_paths) \n",
    "    ta = read_streamlinexr_beam_timeangles(file_path)\n",
    "    get_all_file_start_end_idxs(ta)\n",
    "end\n",
    "\n",
    "\"index of the file for which the index x falls between fi1 and filast\"\n",
    "thisfile_idx(x, fi1, filast) = findfirst( fi1.<= x .<=filast)\n",
    "\n",
    "# this method superseded by one that uses fi1, filast in calling scope\n",
    "idx_beams_in_file(i, ibeam1, fi1, filast) = i - ibeam1[thisfile_idx(i, fi1, filast)] + 1\n",
    "\n",
    "function thisfile_start_end(x, fi1, filast)\n",
    "    j = thisfile_idx(x, fi1, filast)\n",
    "    return fi1[j], filast[j]\n",
    "end\n",
    "\n",
    "\"get name of a file corresponding to a time or index\"\n",
    "thisfile_name(x::Integer, fi1, filast, files=fullfiles) = files[thisfile_idx(x, fi1, filast)]\n",
    "function thisfile_name(dtx::DateTime, dt::Vector{DateTime}, files=fullfiles) \n",
    "    fi1, filast = get_all_file_start_end_idxs(files)\n",
    "    thisfile_name( findlast(dt .<= dtx), fi1, filast, files)\n",
    "end\n",
    "\n",
    "\"read chunks inclusive between [firstdtq lastdq] from vector of files\"\n",
    "function read_lidar_chunks(files, firstdtq, lastdtq)\n",
    "    # read all times\n",
    "    ta, hdr, nbeams = read_lidar.read_streamlinexr_beam_timeangles(files) # 5 s per day\n",
    "\n",
    "    # TODO append day hour to the decimal hour...\n",
    "    hdr[:start_time]\n",
    "    # finding indices\n",
    "    ien, ist = all_gaps(ta[:time]) # identify indices of chunks from gaps\n",
    "    ii, jj = all_start_end_indices(ien, ist)\n",
    "\n",
    "    # Request some beam times. Low-level query takes indices.\n",
    "    # # find indices qst, qen for the request\n",
    "    # dt = @. DateTime(2024,5,31) + Millisecond(round(Int64, ta[:time] * 3_600_000)) # ta[:time] is decimal hour\n",
    "    # # problem: date not defined in ta\n",
    "    # qst = findfirst(dt .>= DateTime(2024,5,31,5))\n",
    "    # qen = findlast( dt .<  DateTime(2024,5,31,10))\n",
    "    qst = findfirst(dt .>= firstdtq)\n",
    "    qen = findlast( dt .<  lastdtq)\n",
    "\n",
    "    # request the first and last chunks\n",
    "    (ist1,ien1, lastist,lastien) = query_start_end_chunks(qst, qen; ien=ien, ist=ist)\n",
    "    # request start and end of all the chunks between ist1 and lastien\n",
    "    istsub, iensub = all_chunks(ist1, lastien)\n",
    "\n",
    "    # sort the chunks by file\n",
    "\n",
    "    # Subset the data within the file to the reuqested chunks\n",
    "    # could be done by skipping to start_beam, and stopping at stop_beam.\n",
    "    ibeam1 = ta[:ibeam1]\n",
    "    # not needed:\n",
    "    ## idx_beams_in_file(i, ibeam1) = i - ibeam1[thisfile_idx(i, ist1, lastien)] + 1\n",
    "    # start_beam = idx_beams_in_file(ist1, ibeam1, ist1, lastien) # default = 1\n",
    "    # stop_beam = idx_beams_in_file(lastien, ibeam1, ist1, lastien)      # default = ta[:nbeams]\n",
    "    \n",
    "    # only ever truncate the data read by a file because of a\n",
    "    # intentional scientific user-level query\n",
    "\n",
    "    # @show ist1, lastien\n",
    "    # @show [fi1 filast]\n",
    "\n",
    "    # read the beam data between ist1 and lastien\n",
    "    beams, h, nbeams0 = read_lidar.read_streamlinexr_stare(files, fi1, filast, ist1, lastien)\n",
    "    return beams, h, ien, ist\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `Day` not defined in `Main.stare`\nSuggestion: check for spelling errors or missing imports.\nHint: a global variable of this name also exists in Dates.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `Day` not defined in `Main.stare`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "Hint: a global variable of this name also exists in Dates.\n",
      "\n",
      "Stacktrace:\n",
      " [1] hour_beams(time::Vector{Union{Missing, Float32}}, ibeam1::Vector{Union{Missing, Int32}}, start_time::Vector{Union{Missing, DateTime}})\n",
      "   @ Main.stare ~/Projects/ASTRAL/lidar/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W4sZmlsZQ==.jl:25\n",
      " [2] hour_beams(ta::Dict{Symbol, Vector})\n",
      "   @ Main.stare ~/Projects/ASTRAL/lidar/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W4sZmlsZQ==.jl:29\n",
      " [3] top-level scope\n",
      "   @ ~/Projects/ASTRAL/lidar/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W6sZmlsZQ==.jl:25"
     ]
    }
   ],
   "source": [
    "# example to get indices of start and end of chunks for one day\n",
    "# tests functions proposed to MOVE to module read_lidar.chunks\n",
    "\n",
    "# include(\"read_lidar.jl\") # reloads readers\n",
    "\n",
    "# set up\n",
    "datapath = joinpath.(pwd(),\"data\",\"20240531\")\n",
    "files = filter(startswith(\"Stare\"), readdir(datapath))\n",
    "fullfiles = joinpath.(datapath, files)\n",
    "\n",
    "# to also read the first hour of the next day\n",
    "dtstamp = datapath[end-7:end]\n",
    "nextdt = DateTime(datapath[end-7:end], dateformat\"yyyymmdd\") + Day(1)\n",
    "nextdatapath = joinpath.(pwd(), \"data\", Dates.format(nextdt, \"yyyymmdd\"))\n",
    "hour00 = readdir( nextdatapath ) |> filter(startswith(\"Stare\")) |> filter(endswith(\"_00.hpl\"))\n",
    "full25files = [fullfiles ; joinpath(nextdatapath, hour00[1])]\n",
    "\n",
    "\n",
    "# read all times\n",
    "ta, hdr, nbeams = read_lidar.read_streamlinexr_beam_timeangles(full25files) # 5 s per day\n",
    "# ta\n",
    "# times, ibeam1, nbeams, h = read_lidar.read_streamlinexr_beam_times(fullfiles) # 5 s per day DEPRECATED\n",
    "fi1, filast = get_all_file_start_end_idxs(ta)\n",
    "\n",
    "hourbeams = hour_beams(ta) # decimal hour of each beam\n",
    "hourbeams[1:5] * 3600 # show seconds since start of day\n",
    "\n",
    "# find wrapped indices\n",
    "i20 = findfirst(ta[:time] .> 20.0)\n",
    "wrap = (i20-1) .+ findall( ta[:time][i20:end] .< 5 )\n",
    "tatime = ta[:time]\n",
    "# increment wrapped times by 24 h\n",
    "tatime[wrap] .+= 24.0\n",
    "# plot(tatime); gcf()\n",
    "\n",
    "# find indices\n",
    "ien, ist = all_gaps(tatime) # identify indices of chunks from gaps\n",
    "ii, jj = all_start_end_indices(ien, ist) ## jj is a LinearIndices\n",
    "\n",
    "# Request some beam times. Low-level query takes indices.\n",
    "# find indices qst, qen for the request\n",
    "dt = @. DateTime(2024,5,31) + Millisecond(round(Int64, ta[:time] * 3_600_000)) # ta[:time] is decimal hour\n",
    "# time wraps around at 0 UTC\n",
    "# problem: date not defined in ta\n",
    "qst = findfirst(dt .>= DateTime(2024,5,31,5))\n",
    "qen = findlast( dt .<  DateTime(2024,5,31,10))\n",
    "# try ones that wrap at 00Z\n",
    "qst = findfirst(dt .>= DateTime(2024,5,31,20))\n",
    "qen = findlast( dt .<  DateTime(2024,6, 1, 0,10)) # 10 min should be enough\n",
    "\n",
    "# get first and last indices of requested chunks\n",
    "(ist1,ien1, lastist,lastien) = query_start_end_chunks(qst, qen; ien=ien, ist=ist)\n",
    "# request start and end of all the chunks between ist1 and lastien\n",
    "istsub, iensub = all_chunks(ist1, lastien, ii)\n",
    "\n",
    "# sort the chunks by file\n",
    "\n",
    "# Subset the data within the file to the reuqested chunks\n",
    "# could be done by skipping to start_beam, and stopping at stop_beam.\n",
    "ibeam1 = ta[:ibeam1]\n",
    "# idx_beams_in_file(i, ibeam1) = i - ibeam1[thisfile_idx(i, fi1, filast)] + 1\n",
    "# start_beam = idx_beams_in_file(ist1   , ibeam1, fi1, filast) # default = 1\n",
    "# stop_beam  = idx_beams_in_file(lastien, ibeam1, fi1, filast) # default = ta[:nbeams]\n",
    "# only ever truncate the data read by a file because of a\n",
    "# intentional scientific user-level query\n",
    "\n",
    "# @show ist1, lastien\n",
    "# @show [fi1 filast]\n",
    "\n",
    "# read the beam data between ist1 and lastien\n",
    "beams, h, nbeams0 = read_lidar.read_streamlinexr_stare(full25files, fi1, filast, ist1, lastien)\n",
    "beams[:dopplervel]\n",
    "\n",
    "# show timeangles Dict\n",
    "ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunks do not end exactly at hourly files, or even at the turnover \n",
    "of the day. A way to deal with this is \n",
    " - Read `timeangles` in daily batches. \n",
    " - The queried data starts at 00 of the day, and ends at the end of the day,\n",
    "   or 00:00:00 of day+1.\n",
    " - Daily batches additionally include the following 00 hour of the next day \n",
    "   - This will provide for reading the\n",
    "   end of the last chunk that runs into the next day.\n",
    " - Read only queried data; there will be minimal overlap for reading \n",
    "   some of the last chunk in the 00 h day+1 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `lastien` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `lastien` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Projects/ASTRAL/lidar/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X11sZmlsZQ==.jl:12"
     ]
    }
   ],
   "source": [
    "# test identifying stare starts from a day of data\n",
    "length(ta[:time]) # == 77024\n",
    "ta[:start_time] # hourly start times\n",
    "# plot(ta[:elevangle],marker=\".\", linestyle=\"none\") # all stares ~90 degree elev\n",
    "starestart = findall(diff(ta[:time]).>(20/3600)) .+ 1\n",
    "\n",
    "clf()\n",
    "plot(ta[:time], marker=\".\", markersize=0.2, linestyle=\"none\") # hour offsets from day\n",
    "plot(starestart, ta[:time][starestart], marker=\".\", markersize=2, color=\"r\", linestyle=\"none\") # hour offsets from day\n",
    "# xlim([0, 7200])\n",
    "# ylim([0, 2])\n",
    "xlim([64000, lastien+60])\n",
    "ylim([20, 25])\n",
    "gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "967-element Vector{DateTime}:\n",
       " 2024-04-28T00:00:00\n",
       " 2024-04-28T01:00:00\n",
       " 2024-04-28T02:00:00\n",
       " 2024-04-28T03:00:00\n",
       " 2024-04-28T04:00:00\n",
       " 2024-04-28T05:00:00\n",
       " 2024-04-28T06:00:00\n",
       " 2024-04-28T07:00:00\n",
       " 2024-04-28T08:00:00\n",
       " 2024-04-28T09:00:00\n",
       " ⋮\n",
       " 2024-06-12T22:00:00\n",
       " 2024-06-12T23:00:00\n",
       " 2024-06-13T00:00:00\n",
       " 2024-06-13T01:00:00\n",
       " 2024-06-13T02:00:00\n",
       " 2024-06-13T03:00:00\n",
       " 2024-06-13T04:00:00\n",
       " 2024-06-13T05:00:00\n",
       " 2024-06-13T06:00:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get all the files, and all the unique hours of the files\n",
    "allstarefiles = vcat( [ joinpath.(\"data\",F, \n",
    "    filter( startswith(r\"Stare_\"), readdir(joinpath(\"data\",F)) ) ) \n",
    "  for F in filter( startswith(r\"20240\"), readdir(\"data\") ) ]... )\n",
    "\n",
    "REm = match.(r\"Stare_116_(\\d{8}_\\d{2}).hpl\", allstarefiles)\n",
    "dth = [ DateTime(r[1], dateformat\"yyyymmdd_HH\") for r in REm ]\n",
    "unique(floor.(dth, Hour)) # all 991 are already unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## line up mean vertical velocity with platform heave\n",
    "using VectorNav - POSMV analysis in [`vectornav.ipynb`](vectornav.ipynb)\n",
    "\n",
    "Timing between POSMV and VectorNav (VN) is determined in \n",
    "[`rot_sandbox.ipynb`](rot_sandbox.ipynb).\n",
    "\n",
    "Use \n",
    "  - POSMV GPS time \n",
    "and \n",
    "  - VN computer time\n",
    "\n",
    "No timing adjustments help or are needed for POSMV and VN then agree to within ~±0.5s. This applies to both legs 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data sets\n",
    "\n",
    "Procedures will differ for different legs. \n",
    "\n",
    "## Timing issues\n",
    "\n",
    "### Leg 1 \n",
    "Has VectorNav data, presumably physically aligned with the coordinate system of the lidar. At least its heave is aligned with the vertical lidar beams.\n",
    "\n",
    "VectorNav clock drifts a few seconds and then corrects, or precesses, with a 51-hour cycle, compared to the POSMV. (I said 13-h cycle, but the figure shows a ~51 h cycle.)\n",
    "\n",
    "2025 Feb 18: The VN computer clock and POSMV GPS clocks agree. See above and \n",
    "[`rot_sandbox.ipynb`](rot_sandbox.ipynb).\n",
    "\n",
    "!OLD!\n",
    "![POSMV](./Vn-POSMV_lag.png \"VectorNav-POSMV lag\")\n",
    "\n",
    "The POSMV clock runs slow compared to its own internal GPS clock.\n",
    "A large time offset accumulates in leg 1.\n",
    "In leg 2 the clock resets to the GPS quasi-regularly every ~2 days. \n",
    "We reconstruct the POSMV time axis\n",
    "to minimize the difference with the GPS (red). \n",
    "This agrees with the VectorNav (except for 18 s GPS leapseconds).\n",
    "\n",
    "The VectorNav time is in GPS convention and the POSMV is in UTC convention, resulting in the 18 s offset due to the GPS leapseconds.\n",
    "\n",
    "![POSMV](./POSMV_pashr-gps.png \"VectorNav-POSMV lag\")\n",
    "\n",
    "### Leg 2, part 1\n",
    "There is no VectorNav data until June 4, when data starts.\n",
    "There is data from the ship's POSMV. Synchronization of pitch and roll\n",
    "is demonstrated in `vectornav.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D2_rho_stare"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# functions for epsilon from stare w\n",
    "# called in loop\n",
    "\n",
    "# timing functions\n",
    "\n",
    "\"\"\"\n",
    "get_time_shift(mdv, heave) positive result means mdv clock is fast.\n",
    "sync by subtracting this lag (in 1020 millisecods) from stare_dt.\n",
    "\"\"\"\n",
    "function get_time_shift(mdv, heave)\n",
    "    # filter to make xcorr work better\n",
    "    xc = xcorr(hp(mdv[:]), hp(heave[:]))\n",
    "    # plot(-(length(mdv)-1):length(mdv)-1, xc )\n",
    "    return argmax(xc) - length(mdv)\n",
    "end\n",
    "\n",
    "\"hourly files -> chunk time indices\"\n",
    "function read_stare_time( St )\n",
    "    # Lidar clock is fast (ahead) by 126832 milliseconds compared to the GPS.\n",
    "    # Moving the timeseries backward (lagging the lidar) compensates its clock error.\n",
    "    # adjust the lidar clock backward to agee with the GPS clock.\n",
    "    lidar_clock_fast_by = Millisecond( 126832 ) # first adjustment\n",
    "    stare_dt = @. (\n",
    "        DateTime(Date(dt)) \n",
    "        + Millisecond(round(Int64, St[:time] * 3_600_000 )) \n",
    "        .- lidar_clock_fast_by ) # 3202\n",
    "\n",
    "    # split into individual stare chunks\n",
    "    pickets = findall( t -> t>Second(30), diff(stare_dt) )\n",
    "    # st = [1; pickets.+1] # ignore start and end of file with a split chunk\n",
    "    # en = [pickets; length(stare_dt)]\n",
    "    st_chunk = pickets[1:end-1] .+ 1\n",
    "    en_chunk = pickets[2:end]\n",
    "    # subdivide into shorter chunks???\n",
    "    return st_chunk, en_chunk\n",
    "end\n",
    "\n",
    "# \"subdivide interval [st en] into fac even intervals\"\n",
    "# subdivide(st,en, fac) = round(Integer, st .+ (st-en)/fac .* [0:fac])\n",
    "\"subdivide single interval [st en] into fac even intervals\"\n",
    "subdivide(st,en, fac) = @. round(Integer, st + ((en-st)*(0:fac)/fac))\n",
    "# # test\n",
    "# subdivide(1,240, 4)\n",
    "\n",
    "\"read and interpolate data to stare chunks\"\n",
    "function read_stare_chunk(St, Vn, UV, st, en )\n",
    "    # time: truncate the file's datestamp to Date, add the decimal hour\n",
    "    stare_dt_raw = @. DateTime(Date(dt)) + Millisecond(round(Int64, St[:time] * 3_600_000 )) # 3202\n",
    "    lidar_clock_fast_by = Millisecond( 126832 ) # adjust for lidar clock fast (ahead) by 126832 milliseconds compared to the GPS.\n",
    "    stare_dt = stare_dt_raw .- lidar_clock_fast_by\n",
    "\n",
    "    # dopplervel (time, z) masked by intensity\n",
    "    dopplervel = masklowi.(St[:dopplervel][st:en,1:ntop], St[:intensity][st:en,1:ntop])\n",
    "    mdv = missmean(dopplervel, dims=2)[:] # conditional mean can have biases\n",
    "\n",
    "    # interpolate Ur,Vr, heave to the lidar stare grid\n",
    "    ind = findindices( stare_dt[st:en], Vn[\"time\"] )\n",
    "    pitch = indavg( Vn[\"pitch\"], ind) # 11-point mean\n",
    "    roll  = indavg( Vn[\"roll\" ], ind)\n",
    "    heave = indavg( Vn[\"heave\"], ind)\n",
    "    # resync the clock to the VactorNav heave - brittle\n",
    "    stare1dt = stare_dt[st:en] # subset\n",
    "    ind = findindices( stare1dt, Vn[\"time\"] )\n",
    "    heave = Vn[\"heave\"][ind]\n",
    "    shift = get_time_shift(mdv[:],heave[:])\n",
    "    # interpolate for the updated synced time\n",
    "    stare1dt .-= Millisecond((1020-80)*shift)\n",
    "    ind = findindices( stare1dt, Vn[\"time\"] ) # this works\n",
    "    heave = indavg( Vn[\"heave\"], ind)\n",
    "\n",
    "    # mean relative velocity\n",
    "    Ur = zeros(size(dopplervel))\n",
    "    Vr = zeros(size(dopplervel))\n",
    "    ind = findindices( Dates.value.(stare1dt), Dates.value.(UV[\"time\"]))\n",
    "    # result must be 1:1 for stare1dt and ind\n",
    "    ls = length(stare1dt)\n",
    "    li = length(ind)\n",
    "    if li < ls # extend ind with last index of UV\n",
    "        ind = [ind; length(UV[\"time\"]).+zeros(Int32, ls-li)]\n",
    "    end\n",
    "    for ih in 1:ntop # loop to broadcast to consistent size\n",
    "        Ur[:,ih] .= UV[:ur][ind,ih]\n",
    "        Vr[:,ih] .= UV[:vr][ind,ih]\n",
    "    end\n",
    "\n",
    "    # questionable: fill all the mean relative velocities\n",
    "    isf = isfinite.(Vr)\n",
    "    Vr[.!isf] .= mean(Vr[isf])\n",
    "    Ur[.!isf] .= mean(Ur[isf])\n",
    "    \n",
    "    return dopplervel, pitch, roll, heave, Ur, Vr, mdv\n",
    "end\n",
    "\n",
    "function filter_vel_coherent_heave( dopplervel, pitch, roll, heave, mdv )\n",
    "    # get the component of dopplervel coherent with heave,pitch,roll\n",
    "    # allowing for phase shifts\n",
    "    mdv_clean_heave, _ = remove_coherent_component( Float64.(mdv), Float64.(heave) )\n",
    "    mdv_remove = mdv .- mdv_clean_heave # works\n",
    "    return dopplervel .- mdv_remove\n",
    "end\n",
    "\n",
    "# for testing\n",
    "# seconds since dt0\n",
    "\"offset seconds since dt0\"\n",
    "toffs(dt, dt0=DateTime(2024,4,1)) = Millisecond( dt - dt0 ).value / 1000 # seconds\n",
    "\"datetime by adding time in seconds to dt0. Inverse of toffs\"\n",
    "tons(toffs, dt0=DateTime(2024,4,1)) = Millisecond(round(Int64, 1000*toffs)) + dt0\n",
    "\n",
    "#= usage\n",
    "vntoffs = toffs.( Vn[\"time\"] )\n",
    "# test indavg\n",
    "vndt_int = tons.(indavg( vntoffs, ind ))\n",
    "=#\n",
    "\n",
    "function plot_stare_motcor( height, dopplervel, mdv, mdv_remove, pitch, roll, Ur, Vr)\n",
    "    mm = @. minute(stare1dt) + (second(stare1dt) + millisecond(stare1dt)/1000 )/60\n",
    "\n",
    "    clf()\n",
    "    subplot(3,1,1)\n",
    "    pcolormesh(mm, height[1:ntop]/1e3, m2n.(pd(dopplervel.-mdv)), cmap=ColorMap(\"RdYlBu_r\"), vmin=-1, vmax=1)\n",
    "    colorbar()\n",
    "    title(\"vel - mdv\")\n",
    "    ylim([0, 1])\n",
    "\n",
    "    subplot(3,1,2)\n",
    "    pcolormesh(mm, height[1:ntop]/1e3, m2n.(pd(dopplervel.-mdv_remove)), cmap=ColorMap(\"RdYlBu_r\"), vmin=-1, vmax=1)\n",
    "    colorbar()\n",
    "    title(\"vel - mdv coherent with heave\")\n",
    "    ylim([0, 1])\n",
    "\n",
    "    subplot(3,1,3)\n",
    "    w = wtrue.(dopplervel.-heave, Ur, Vr, pitch*pi/180, roll*pi/180)\n",
    "    pcolormesh(mm, height[1:ntop]/1e3, m2n.(pd(w)), cmap=ColorMap(\"RdYlBu_r\"), vmin=-1, vmax=1)\n",
    "    colorbar()\n",
    "    title(\"w heave and tilt angle decompostion\")\n",
    "    ylim([0, 1])\n",
    "\n",
    "    tight_layout()\n",
    "    # Corrections in wtrue (other than adding the -heave) do not seem to be important\n",
    "    # at this time. Motion compensation is probably working, then.\n",
    "    return gcf()\n",
    "end\n",
    "\n",
    "## structure function dissipation\n",
    "\n",
    "# stucture function constants\n",
    "C2ll = 2.0\n",
    "epsilon(A) = sqrt(3/4 * A/C2ll)^3\n",
    "# struf(epsilon, r,r1) = C2ll * epsilon^(2/3) * r^(2/3) * (4 - (r1/r)^2)/3\n",
    "# instruf(w1,w2) = (w1-w2)^2\n",
    "# rho(r1,r) = r^(2/3) * (1 - ((r1/r)^2)/4)\n",
    "# zmid(z1,z2) = (z1 + z2) / 2\n",
    "# plot bin averaged instruf vs rho\n",
    "# fit \n",
    "# D = A*rho + noise\n",
    "# for A and noise\n",
    "# A = 4/3 * C2ll * epsilon^(2/3)\n",
    "\n",
    "\"bin average D2 in equally-populated bins of rho\"\n",
    "function equal_bin(rho, D2; nbin=200, nbin_out_max=17 )\n",
    "    ii = findall(.!ismissing.(rho) .& .!ismissing.(D2) )\n",
    "    nrho = length(ii)\n",
    "    if nrho >= 20\n",
    "        sp = sortperm(rho[ii])\n",
    "        srho = rho[ii][sp]\n",
    "        step = max(1,round(Int32,nrho/nbin))\n",
    "        rhobin = [ 0; rho[ii][sp[step:step:nrho]] ]\n",
    "        jj = findall(.!ismissing.(rhobin) .& isfinite.(rhobin))\n",
    "        D2inbin = binavg(D2[ii], rho[ii], rhobin[jj])\n",
    "        rhoinbin = binavg(rho[ii], rho[ii], rhobin[jj])\n",
    "        nbin_out = min(nbin_out_max, length(rhobin))\n",
    "        return nbin_out, rhobin[1:nbin_out], D2inbin[1:nbin_out], rhoinbin[1:nbin_out]\n",
    "    else\n",
    "        return 1, [missing], [missing], [missing]\n",
    "    end\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "structure function D2, rho, A, epsilon at each level from w stare\n",
    "D2bin, rhobin, A, noise = D2_rho_stare( w, pitch, roll, Ur, Vr; out=17 )\n",
    "\"\"\"\n",
    "function D2_rho_stare( w, pitch, roll, Ur, Vr; nbin_out_max=17 )\n",
    "\n",
    "    nbin_out = nbin_out_max\n",
    "    \n",
    "    (nt, nz) = size(w)\n",
    "    A      = Vector{Union{Missing,Float64}}(missing, nz)\n",
    "    noise  = Vector{Union{Missing,Float64}}(missing, nz)\n",
    "    rhobin = Matrix{Union{Missing,Float64}}(missing, nbin_out, nz)\n",
    "    D2bin  = Matrix{Union{Missing,Float64}}(missing, nbin_out, nz)\n",
    "    for izo in 1:nz # loop vertically\n",
    "        #=\n",
    "        ci1,ci2, li1,li2, it1,iz1,it2,iz2 = lidarindices(nt, nz, izo) # might do outside the loop\n",
    "        zm, dr2, dz2, D2 = displacements( ci1,ci2, Ur*timestep,Vr*timestep,\n",
    "                                          pitch,roll, w; timestep=timestep )\n",
    "        rho = rhopair.(dr2, dz2) # approx r^2/3\n",
    "        # bin average str fcn D2 in equally-populated bins of rho\n",
    "        @show size(rho), size(D2)\n",
    "        rhobin_, D2inbin_, rhoinbin_ = equal_bin(rho, D2)\n",
    "        rhobin[:,izo] .= rhoinbin_[1:nbin_out]\n",
    "        D2bin[ :,izo] .= D2inbin_[ 1:nbin_out]\n",
    "        # regress to get A\n",
    "        ii = .!ismissing.(rhobin[:,izo]) .& .!ismissing.(D2bin[:,izo])\n",
    "        if sum(ii) > 2\n",
    "            A[izo] = anom(rhobin[:,izo][ii]) \\ anom(D2bin[:,izo][ii])\n",
    "            noise[izo] = mean(D2bin[:,izo][ii]) - A[izo] * mean(rhobin[:,izo][ii]) # noise\n",
    "        end\n",
    "        =#\n",
    "        ci1,ci2, li1,li2, it1,iz1,it2,iz2 = lidarindices(nt, nz, izo) # might do outside the loop\n",
    "        zm, dr2, dz2, D2 = displacements( ci1,ci2, Ur*timestep,Vr*timestep,\n",
    "                                          pitch,roll, w; timestep=timestep )\n",
    "        rho = rhopair.(dr2, dz2) # approx r^2/3\n",
    "        # bin average str fcn D2 in equally-populated bins of rho\n",
    "        nbin_actual, rhobin_, D2inbin_, rhoinbin_ = equal_bin(rho, D2; nbin_out_max=nbin_out_max)\n",
    "        rhobin[1:nbin_actual,izo] .= rhoinbin_\n",
    "        D2bin[ 1:nbin_actual,izo] .= D2inbin_\n",
    "        # regress to get A\n",
    "        ii = .!ismissing.(rhobin[1:nbin_actual,izo]) .& .!ismissing.(D2bin[1:nbin_actual,izo])\n",
    "        if sum(ii) > 2\n",
    "            A[izo] = anom(rhobin[1:nbin_actual,izo][ii]) \\ anom(D2bin[1:nbin_actual,izo][ii])\n",
    "            noise[izo] = mean(D2bin[1:nbin_actual,izo][ii]) - A[izo] * mean(rhobin[1:nbin_actual,izo][ii]) # noise\n",
    "        end\n",
    "    end\n",
    "    return D2bin, rhobin, A, noise\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^test D2_rho_stare line by line\n",
    "#=\n",
    "(nt, nz) = size(w)\n",
    "nbin_out_max = 17\n",
    "A      = Vector{Union{Missing,Float64}}(missing, nz)\n",
    "noise  = Vector{Union{Missing,Float64}}(missing, nz)\n",
    "rhobin = Matrix{Union{Missing,Float64}}(missing, nbin_out, nz)\n",
    "D2bin  = Matrix{Union{Missing,Float64}}(missing, nbin_out, nz)\n",
    "\n",
    "izo = 1\n",
    "    ci1,ci2, li1,li2, it1,iz1,it2,iz2 = lidarindices(nt, nz, izo) # might do outside the loop\n",
    "    zm, dr2, dz2, D2 = displacements( ci1,ci2, Ur*timestep,Vr*timestep,\n",
    "                                      pitch*pi/180,roll*pi/180, w; timestep=timestep ) #hacked\n",
    "    rho = rhopair.(dr2, dz2) # approx r^2/3\n",
    "    # bin average str fcn D2 in equally-populated bins of rho\n",
    "    nbin_actual, rhobin_, D2inbin_, rhoinbin_ = equal_bin(rho, D2; nbin_out_max=nbin_out_max)\n",
    "    rhobin[1:nbin_actual,izo] .= rhoinbin_\n",
    "    D2bin[ 1:nbin_actual,izo] .= D2inbin_\n",
    "    # regress to get A\n",
    "    # this is a mess because the bins can be of different sizes, \n",
    "    # resulting in different quality estimates of epsilon\n",
    "    ii = .!ismissing.(rhobin[1:nbin_actual,izo]) .& .!ismissing.(D2bin[1:nbin_actual,izo])\n",
    "    if sum(ii) > 2\n",
    "        A[izo] = anom(rhobin[1:nbin_actual,izo][ii]) \\ anom(D2bin[1:nbin_actual,izo][ii])\n",
    "        noise[izo] = mean(D2bin[1:nbin_actual,izo][ii]) - A[izo] * mean(rhobin[1:nbin_actual,izo][ii]) # noise\n",
    "    end\n",
    "=#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute TKE dissipation in ~10 min chunks by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# line by line test one chunk\n",
    "\n",
    "ntop = 80       # subset vertical levels\n",
    "timestep = 1.02 # s\n",
    "lidarstemdir = \"./data\" # \"/Users/deszoeks/Data/EKAMSAT/lidar\"\n",
    "lidardaydirs = filter( startswith(\"2024\"), readdir(lidarstemdir) )\n",
    "\n",
    "lidardaydir = lidardaydirs[2]\n",
    "dt = Date(lidardaydir, dateformat\"yyyymmdd\")\n",
    "\n",
    "epsi_tmp = Matrix{Union{Missing,Float64}}(missing, 6,ntop)\n",
    "\n",
    "Vn = read_daily_Vn( dt )            # Dict\n",
    "# load daily relative horizontal winds\n",
    "UV = get_daily_meanuv( dt )\n",
    "\n",
    "lidarfile = filter(startswith(\"Stare_116_\"), readdir(joinpath(lidarstemdir,lidardaydir)))[21]\n",
    "splt = split(lidarfile, r\"[_.]\")\n",
    "dt = DateTime(splt[3]*splt[4], dateformat\"yyyymmddHH\") # a DateTime\n",
    "\n",
    "St, _ = read_streamlinexr_stare( dt )\n",
    "height = St[:height][1:ntop]\n",
    "# TO DO: read the next hour to get continuous chunk at end ...\n",
    "st_chunks, en_chunks = read_stare_time( St )\n",
    "ichunk = 5\n",
    "st = st_chunks[ichunk]\n",
    "    en = en_chunks[ichunk]\n",
    "    # read a chunk\n",
    "    dopplervel, pitch, roll, heave, Ur, Vr, mdv = read_stare_chunk( St, Vn, UV, st, en )\n",
    "    # cannot consistently sync heave and dopplervel\n",
    "    #w = filter_vel_coherent_heave( dopplervel, pitch*pi/180, roll*pi/180, heave, mdv )\n",
    "    w = dopplervel .- mdv \n",
    "    # subplot(2,1,1)\n",
    "    # pcolormesh(pd(m2n.(w)), cmap=ColorMap(\"RdYlBu_r\"), vmin=-1, vmax=1)\n",
    "    D2bin, rhobin, A, noise = D2_rho_stare( w, pitch*pi/180, roll*pi/180, Ur, Vr )\n",
    "    epsi_tmp[ichunk,:] = @. epsilon(max(0,A))\n",
    "\n",
    "    subplot(2,1,1)\n",
    "    plot(m2n.(rhobin), m2n.(D2bin), marker=\".\", linewidth=0.4)\n",
    "    xlim([0, 50])\n",
    "    ylim([0, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: compute dissipation for one hour\n",
    "\n",
    "ntop = 80       # subset vertical levels\n",
    "timestep = 1.02 # s\n",
    "lidarstemdir = \"/Users/deszoeks/Data/EKAMSAT/lidar\" # \"./data/lidar\"\n",
    "lidardaydirs = filter( startswith(\"2024\"), readdir(lidarstemdir) )\n",
    "\n",
    "lidardaydir = lidardaydirs[5]\n",
    "dt = Date(lidardaydir, dateformat\"yyyymmdd\")\n",
    "\n",
    "epsi_tmp = Matrix{Union{Missing,Float64}}(missing, 6,ntop)\n",
    "\n",
    "Vn = read_daily_Vn( dt )            # Dict\n",
    "# load daily relative horizontal winds\n",
    "UV = get_daily_meanuv( dt )\n",
    "\n",
    "lidarfile = filter(startswith(\"Stare_116_\"), readdir(joinpath(lidarstemdir,lidardaydir)))[12]\n",
    "dt = Date(lidardaydir, dateformat\"yyyymmdd\")\n",
    "epsi_tmp = Matrix{Union{Missing,Float64}}(missing, 6,ntop)\n",
    "\n",
    "Vn = read_daily_Vn( dt )            # Dict\n",
    "# load daily relative horizontal winds\n",
    "UV = get_daily_meanuv( dt )\n",
    "\n",
    "lidarfile = filter(startswith(\"Stare_116_\"), readdir(joinpath(lidarstemdir,lidardaydir)))[12]\n",
    "splt = split(lidarfile, r\"[_.]\")\n",
    "dt = DateTime(splt[3]*splt[4], dateformat\"yyyymmddHH\") # a DateTime\n",
    "\n",
    "St, _ = read_streamlinexr_stare( dt )\n",
    "height = St[:height][1:ntop]\n",
    "# TO DO: read the next hour to get continuous chunk at end ...\n",
    "st_chunks, en_chunks = read_stare_time( St )\n",
    "for (ichunk, st) in enumerate(st_chunks)\n",
    "    en = en_chunks[ichunk]\n",
    "    # read a chunk\n",
    "    dopplervel, pitch, roll, heave, Ur, Vr, mdv = read_stare_chunk( St, Vn, UV, st, en )\n",
    "    if any(isfinite.(Ur)) && any(isfinite.(Vr))\n",
    "        # cannot consistently sync heave and dopplervel\n",
    "        #w = filter_vel_coherent_heave( dopplervel, pitch*pi/180, roll*pi/180, heave, mdv )\n",
    "        w = dopplervel .- mdv \n",
    "        D2bin, rhobin, A, noise = D2_rho_stare( w, pitch*pi/180, roll*pi/180, Ur, Vr )\n",
    "        epsi_tmp[ichunk,:] = @. epsilon(max(0,A))\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8640×80 Matrix{Union{Missing, Float64}}:\n",
       " missing  missing  missing  missing  …  missing  missing  missing  missing\n",
       " missing  missing  missing  missing     missing  missing  missing  missing\n",
       " missing  missing  missing  missing     missing  missing  missing  missing\n",
       " missing  missing  missing  missing     missing  missing  missing  missing\n",
       " missing  missing  missing  missing     missing  missing  missing  missing\n",
       " missing  missing  missing  missing  …  missing  missing  missing  missing\n",
       " missing  missing  missing  missing     missing  missing  missing  missing\n",
       " missing  missing  missing  missing     missing  missing  missing  missing\n",
       " missing  missing  missing  missing     missing  missing  missing  missing\n",
       " missing  missing  missing  missing     missing  missing  missing  missing\n",
       " ⋮                                   ⋱                             \n",
       " missing  missing  missing  missing     missing  missing  missing  missing\n",
       " missing  missing  missing  missing     missing  missing  missing  missing\n",
       " missing  missing  missing  missing     missing  missing  missing  missing\n",
       " missing  missing  missing  missing     missing  missing  missing  missing\n",
       " missing  missing  missing  missing  …  missing  missing  missing  missing\n",
       " missing  missing  missing  missing     missing  missing  missing  missing\n",
       " missing  missing  missing  missing     missing  missing  missing  missing\n",
       " missing  missing  missing  missing     missing  missing  missing  missing\n",
       " missing  missing  missing  missing     missing  missing  missing  missing"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loop through lidar data and compute TKE dissipation rate\n",
    "\n",
    "ntop = 80       # subset vertical levels\n",
    "timestep = 1.02 # s\n",
    "lidarstemdir = \"./data\" # \"/Users/deszoeks/Data/EKAMSAT/lidar\"\n",
    "lidardaydirs = filter( startswith(\"2024\"), readdir(lidarstemdir) )\n",
    "epsi = Matrix{Union{Missing,Float64}}(missing, 6*24*60, ntop)\n",
    "# lidardtstart = zeros(6*24*60)\n",
    "# lidardtend = zeros(6*24*60)\n",
    "\n",
    "# compute epsilon for all stares # commented out\n",
    "#=\n",
    "\n",
    "# for lidardaydir in lidardaydirs[2:16] # files available leg 1\n",
    "for lidardaydir in lidardaydirs[2:6] # files available leg 1\n",
    "# for lidardaydir in lidardaydirs[17:end] # files available leg 2\n",
    "    dt = Date(lidardaydir, dateformat\"yyyymmdd\") # a Date\n",
    "    print(\"$(lidardaydir) \")\n",
    "    # can't sync motion with clock!\n",
    "    try # load daily vectornav  \n",
    "        Vn = read_daily_Vn( dt )            # Dict\n",
    "    catch\n",
    "        print(\"no VectorNav for $(dt)\")\n",
    "    end\n",
    "    try # load daily relative horizontal winds\n",
    "        UV = get_daily_meanuv( dt )         # NCDataset\n",
    "    catch\n",
    "        print(\"no mean wind for $(dt)\\n\")\n",
    "        #save epsilon.jld2 just in case\n",
    "        # jldopen(\"epsilon_tmp.jld2\", \"w+\") do file\n",
    "        #     file[\"epsilon\"] = epsi\n",
    "    end\n",
    "\n",
    "    # load hourly lidar stares\n",
    "    bigind = 0 # index time, save daily\n",
    "    for lidarfile in filter(startswith(\"Stare_116_\"), readdir(joinpath(lidarstemdir,lidardaydir)))\n",
    "        splt = split(lidarfile, r\"[_.]\")\n",
    "        dt = DateTime(splt[3]*splt[4], dateformat\"yyyymmddHH\") # a DateTime\n",
    "        print(\"$(splt[4]) \")\n",
    "        \n",
    "        St, _ = read_streamlinexr_stare( dt )\n",
    "        height = St[:height][1:ntop]\n",
    "        # TO DO: read the next hour to get continuous chunk at end ...\n",
    "        st_chunks, en_chunks = read_stare_time( St )\n",
    "        stare_dt = @. ( dt \n",
    "            + Millisecond(round(Int64, St[:time] * 3_600_000 )) \n",
    "            - lidar_clock_fast_by )\n",
    "        for (ichunk, st) in enumerate(st_chunks)\n",
    "            en = en_chunks[ichunk]\n",
    "            bigind += 1\n",
    "            # read a chunk\n",
    "            dopplervel, pitch, roll, heave, Ur, Vr, mdv = read_stare_chunk( St, Vn, UV, st, en )\n",
    "            if any(isfinite.(Ur)) && any(isfinite.(Vr))\n",
    "                # cannot consistently sync heave and dopplervel\n",
    "                #w = filter_vel_coherent_heave( dopplervel, pitch*pi/180, roll*pi/180, heave, mdv )\n",
    "                w = dopplervel .- mdv \n",
    "                D2bin, rhobin, A, noise = D2_rho_stare( w, pitch*pi/180, roll*pi/180, Ur, Vr )\n",
    "                epsi[bigind,:] .= @. epsilon.(max(0,A))\n",
    "            else\n",
    "                epsi[bigind,:] .= -4 # code for missing wind\n",
    "            end\n",
    "            lidardtstart[bigind] = stare_dt[st]\n",
    "            lidardtend[bigind] = stare_dt[en]\n",
    "        end\n",
    "    end\n",
    "    print(\"\\n\")\n",
    "    #save data daily to epsilon.jld2\n",
    "    fileout = \"epsilon_$(Dates.format(dt, dateformat\"yyyymmdd\")).jld2\"\n",
    "    print(\"Saving $(fileout)\\n\")\n",
    "    jldopen(fileout, \"w+\") do file\n",
    "        file[\"epsilon\"] = epsi[1:bigind,:]\n",
    "        file[\"start_dt\"] = lidardtstart[1:bigind]\n",
    "        file[\"end_dt\"]   = lidardtend[1:bigind]\n",
    "    end\n",
    "end\n",
    "=#\n",
    "\n",
    "# notes\n",
    "# heave channel constant on 2024-5-12\n",
    "# no mean wind on 2024-05-12\n",
    "# 20240519 no VectorNav for 2024-05-19 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `Millisecond` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nHint: a global variable of this name may be made accessible by importing Dates in the current active module Main",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `Millisecond` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "Hint: a global variable of this name may be made accessible by importing Dates in the current active module Main\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Projects/ASTRAL/lidar/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X25sZmlsZQ==.jl:3"
     ]
    }
   ],
   "source": [
    "# just compute and save start and end times for stares\n",
    "\n",
    "lidar_clock_fast_by = Millisecond( 126832 ) # ~2 minutes; check this!\n",
    "\n",
    "lidardtstart = Vector{DateTime}(undef, 6*24*60)\n",
    "lidardtend = Vector{DateTime}(undef, 6*24*60)\n",
    "\n",
    "# for lidardaydir in lidardaydirs[2:16] # files available leg 1\n",
    "for lidardaydir in lidardaydirs[2:4] # files available leg 1\n",
    "        # for lidardaydir in lidardaydirs[17:end] # files available leg 2\n",
    "    dt = Date(lidardaydir, dateformat\"yyyymmdd\") # a Date\n",
    "    print(\"$(lidardaydir) \")\n",
    "    # can't sync motion with clock!\n",
    "    try # load daily vectornav  \n",
    "        Vn = read_daily_Vn(dt)            # Dict\n",
    "    catch\n",
    "        print(\"no VectorNav for $(dt)\")\n",
    "    end\n",
    "    try # load daily relative horizontal winds\n",
    "        UV = get_daily_meanuv(dt)         # NCDataset\n",
    "    catch\n",
    "        print(\"no mean wind for $(dt)\\n\")\n",
    "        #save epsilon.jld2 just in case\n",
    "        # jldopen(\"epsilon_tmp.jld2\", \"w+\") do file\n",
    "        #     file[\"epsilon\"] = epsi\n",
    "    end\n",
    "\n",
    "    # load hourly lidar stares\n",
    "    bigind = 0 # index time, save daily and reset bigind daily\n",
    "    # read each hour in the day\n",
    "    for lidarfile in filter(startswith(\"Stare_116_\"), readdir(joinpath(lidarstemdir, lidardaydir)))\n",
    "        splt = split(lidarfile, r\"[_.]\")\n",
    "        dt = DateTime(splt[3] * splt[4], dateformat\"yyyymmddHH\") # a DateTime\n",
    "        print(\"$(splt[4]):\")\n",
    "\n",
    "        # read 1 hour\n",
    "        St, _ = read_streamlinexr_stare(dt)\n",
    "        st_chunks, en_chunks = read_stare_time(St)\n",
    "        stare_dt = @. ( dt \n",
    "            + Millisecond(round(Int64, St[:time] * 3_600_000 )) \n",
    "            - lidar_clock_fast_by )\n",
    "        for (ichunk, st) in enumerate(st_chunks) # find time of each chunk\n",
    "            en = en_chunks[ichunk]\n",
    "            bigind += 1\n",
    "            lidardtstart[bigind] = stare_dt[st]\n",
    "            lidardtend[bigind]   = stare_dt[en]\n",
    "        end\n",
    "    end\n",
    "    print(\"\\n\")\n",
    "    #save data daily to epsilon.jld2\n",
    "    fileout = \"staredt_$(Dates.format(dt, dateformat\"yyyymmdd\")).jld2\"\n",
    "    print(\"Saving $(fileout)\\n\")\n",
    "    jldopen(fileout, \"w\") do file\n",
    "        file[\"start_dt\"] = lidardtstart[1:bigind]\n",
    "        file[\"end_dt\"]   = lidardtend[1:bigind]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <matplotlib.collections.QuadMesh object at 0x34d60cd60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jldopen(\"./epsilon_data/epsilon_tmp.jld2\", \"r\") do file\n",
    "    epsi = file[\"epsilon\"]\n",
    "end\n",
    "\n",
    "pcolormesh(pd(m2n.(epsi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `stare1dt` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `stare1dt` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Projects/ASTRAL/lidar/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X30sZmlsZQ==.jl:25"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "\n",
    "# check that St and Vn data are synced\n",
    "clf()\n",
    "subplot(2,1,1)\n",
    "plot(hp(mdv))\n",
    "plot(hp(heave))\n",
    "xlim([400, 440])\n",
    "# check lags\n",
    "xc = xcorr( hp(mdv[:]), hp(heave) )\n",
    "subplot(2,1,2)\n",
    "plot( -(length(mdv)-1):length(mdv)-1, xc )\n",
    "xlim([-40, 40])\n",
    "#length(xc), 2*(length(mdv)-1) + 1\n",
    "argmax(xc) - length(mdv) # 0-lag center moved to 0\n",
    "# length(xc), length(mdv), 2*length(heave)\n",
    "\n",
    "\n",
    "plot(mdv)\n",
    "# plot(mdv_clean_heave)\n",
    "# plot(mdv_clean_heavepitch)\n",
    "# plot(mdv_clean_heavepitch)\n",
    "\n",
    "\n",
    "plot(stare1dt, heave)\n",
    "plot(stare1dt, mdv)\n",
    "plot(stare1dt, mdv - heave) # subtract heave because its downward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare the structure function analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `clf` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `clf` not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[1]:1"
     ]
    }
   ],
   "source": [
    "clf()\n",
    "subplot(2,1,1)\n",
    "plot(rhobin, D2bin, marker=\"*\")\n",
    "plot([0, 2.2*mean(rhobin[1:17])], noise .+ A.*[0, 2.2*mean(rhobin[1:17])])\n",
    "xlim([0, 30])\n",
    "xlabel(L\"\\rho = r^{2/3}\"\n",
    "title(\"TKE dissipation = 1.4 $\\times 10^{-4}$\")\n",
    "epsilon(A) # 1.3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
